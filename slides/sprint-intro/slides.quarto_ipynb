{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Skrub sprint\"\n",
        "title-block-banner: true\n",
        "date: 2025-10-29\n",
        "subtitle: \"Women in Machine Learning & Data Science\"\n",
        "author: \"Riccardo Cappuzzo\"\n",
        "institute: \"Probabl\"\n",
        "format: \n",
        "    revealjs:\n",
        "        slide-number: c/t\n",
        "        show-slide-number: all\n",
        "        preview-links: auto\n",
        "        embed-resources: false\n",
        "        transition: slide\n",
        "        theme: simple\n",
        "        logo: images/skrub.svg\n",
        "        css: style.css\n",
        "        footer: \"https://skrub-data.org/skrub-materials/\"\n",
        "incremental: false\n",
        "params: \n",
        "    version: \"base\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## `whoami`  \n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "- I am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub ![](images/inria.png){width=250}\n",
        "\n",
        "- I'm Italian, but I don't drink coffee, wine, and I like pizza with fries ![](images/raora.png){width=50}\n",
        "\n",
        "- I did my PhD in C√¥te d'Azur, but I moved to Paris because it was too sunny and \n",
        "I don't like the sea ![](images/nice.jpg){width=250}\n",
        "\n",
        ":::\n",
        "\n",
        "## Who are you? \n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "- Who has already made contributions in open source? \n",
        "\n",
        "- Who has heard of skrub before today?\n",
        ":::\n",
        "\n",
        "## Roadmap for the presentation\n",
        "\n",
        "- What is skrub\n",
        "- Contributing to skrub: subjects\n",
        "- Contributing to skrub: setting up the environment\n",
        "\n",
        "QR code for the presentation at the end! \n",
        "\n",
        "# What is skrub? \n",
        "\n",
        "::: {.callout-tip}\n",
        "Skrub is a Python library that sits between data stored in dataframes and machine\n",
        "learning with scikit-learn. \n",
        ":::\n",
        "\n",
        "Skrub eases machine learning with dataframes. \n",
        "\n",
        "## {auto-animate=\"true\" } \n",
        "![](images/powerpuff_girls_1.png)\n",
        "\n",
        "## {auto-animate=\"true\" } \n",
        "![](images/powerpuff_girls_2.png)\n",
        "\n",
        "## Skrub compatibility\n",
        "- Skrub is mostly written in Python, but it inlcudes some Javascript\n",
        "- Skrub is fully compatible with pandas and polars\n",
        "    - Any feature needs to be supported by both libraries\n",
        "- Skrub transformers are fully compatible with scikit-learn\n",
        "    - Transformers need to satisfy some requirements\n",
        "\n",
        "## First, an example pipeline\n",
        "1. Gather some data\n",
        "2. Explore the data\n",
        "3. Preprocess the data \n",
        "4. Perform feature engineering \n",
        "5. Build a scikit-learn pipeline\n",
        "6. ???\n",
        "7. Profit?  \n",
        "\n",
        "##  \n",
        "![](images/here-we-go-again.png)\n",
        "\n",
        "## `skrub.TableReport` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "from skrub import TableReport\n",
        "TableReport(employee_salaries)\n",
        "```\n",
        "[TableReport Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link=\"true\"}\n",
        "\n",
        "\n",
        "::: {.fragment}\n",
        "Main features:\n",
        "\n",
        "- Obtain high-level statistics about the data\n",
        "- Explore the distribution of values and find outliers\n",
        "- Discover highly correlated columns \n",
        "- Export and share the report as an `html` file\n",
        ":::\n",
        "\n",
        "\n",
        "## `skrub.TableReport` {auto-animate=\"true\"}\n",
        "\n",
        "- The report uses uses Jinjia templates and Javascript for interactivity\n",
        "- The backend is in Python\n",
        "- Space is limited, need to maximize information density. \n",
        "- Light > feature-rich (no plotly)\n",
        "\n",
        "## Data cleaning with pandas/polars: setup {.smaller auto-animate=\"true\"}\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "be57baf7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "data = {\n",
        "    \"Int\": [2, 3, 2],  # Multiple unique values\n",
        "    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n",
        "    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n",
        "    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n",
        "    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n",
        "    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n",
        "}\n",
        "\n",
        "df_pd = pd.DataFrame(data)\n",
        "display(df_pd)"
      ],
      "id": "986a7175",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "a3199210"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "data = {\n",
        "    \"Int\": [2, 3, 2],  # Multiple unique values\n",
        "    \"Const str\": [\"x\", \"x\", \"x\"],  # Single unique value\n",
        "    \"Str\": [\"foo\", \"bar\", \"baz\"],  # Multiple unique values\n",
        "    \"All nan\": [np.nan, np.nan, np.nan],  # All missing values\n",
        "    \"All empty\": [\"\", \"\", \"\"],  # All empty strings\n",
        "    \"Date\": [\"01 Jan 2023\", \"02 Jan 2023\", \"03 Jan 2023\"],\n",
        "}\n",
        "\n",
        "df_pl = pl.DataFrame(data)\n",
        "display(df_pl)"
      ],
      "id": "a238f06d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Nulls, datetimes, constant columns with pandas/polars {.smaller auto-animate=\"true\"}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "3e59654d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Parse the datetime strings with a specific format\n",
        "df_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d %b %Y')\n",
        "\n",
        "# Drop columns with only a single unique value\n",
        "df_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) > 1]\n",
        "\n",
        "# Function to drop columns with only missing values or empty strings\n",
        "def drop_empty_columns(df):\n",
        "    # Drop columns with only missing values\n",
        "    df_cleaned = df.dropna(axis=1, how='all')\n",
        "    # Drop columns with only empty strings\n",
        "    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n",
        "    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n",
        "    return df_cleaned\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df_pd_cleaned = drop_empty_columns(df_pd_cleaned)"
      ],
      "id": "205e8826",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "6fd1ddcb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true \n",
        "# Parse the datetime strings with a specific format\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.col(\"Date\").str.strptime(pl.Date, \"%d %b %Y\", strict=False).alias(\"Date\")\n",
        "])\n",
        "\n",
        "# Drop columns with only a single unique value\n",
        "df_pl_cleaned = df_pl.select([\n",
        "    col for col in df_pl.columns if df_pl[col].n_unique() > 1\n",
        "])\n",
        "\n",
        "# Import selectors for dtype selection\n",
        "import polars.selectors as cs\n",
        "\n",
        "# Drop columns with only missing values or only empty strings\n",
        "def drop_empty_columns(df):\n",
        "    all_nan = df.select(\n",
        "        [\n",
        "            col for col in df.select(cs.numeric()).columns if \n",
        "            df [col].is_nan().all()\n",
        "        ]\n",
        "    ).columns\n",
        "    \n",
        "    all_empty = df.select(\n",
        "        [\n",
        "            col for col in df.select(cs.string()).columns if \n",
        "            (df[col].str.strip_chars().str.len_chars()==0).all()\n",
        "        ]\n",
        "    ).columns\n",
        "\n",
        "    to_drop = all_nan + all_empty\n",
        "\n",
        "    return df.drop(to_drop)\n",
        "\n",
        "df_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
      ],
      "id": "193d41e8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## `skrub.Cleaner` {.smaller auto-animate=\"true\"}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "a05cd2a2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pd)\n",
        "display(df_cleaned)"
      ],
      "id": "bdcd916a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "af22c7d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d %b %Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pl)\n",
        "display(df_cleaned)"
      ],
      "id": "3ebf0e8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## `skrub.Cleaner` {auto-animate=\"true\"}\n",
        "- The actual transformations are performed in part by `skrub.DropUninformative`.\n",
        "- New criteria for selecting columns should go in `DropUninformative`. \n",
        "\n",
        "\n",
        "## `skrub.DatetimeEncoder` {auto-animate=\"true\" visibility=\"uncounted\" .smaller}"
      ],
      "id": "0358b315"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df = pl.DataFrame(data)"
      ],
      "id": "158ccbfb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import DatetimeEncoder, ToDatetime\n",
        "\n",
        "X_date = ToDatetime().fit_transform(df[\"date\"])\n",
        "de = DatetimeEncoder(resolution=\"second\")\n",
        "# de = DatetimeEncoder(periodic_encoding=\"spline\")\n",
        "X_enc = de.fit_transform(X_date)\n",
        "print(X_enc)"
      ],
      "id": "2c52cf1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding categorical (string/text) features\n",
        "Categorical features have a \"**cardinality**\": the number of unique values\n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "- Low cardinality: `OneHotEncoder`\n",
        "- High cardinality (>40 unique values): `skrub.StringEncoder`\n",
        "- Text: `skrub.TextEncoder` and pretrained models from HuggingFace Hub\n",
        "\n",
        ":::\n",
        "\n",
        "## Encoding _all the features_: `TableVectorizer` { auto-animate=\"true\"}\n"
      ],
      "id": "d52c9c49"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import TableVectorizer\n",
        "\n",
        "table_vec = TableVectorizer()\n",
        "df_encoded = table_vec.fit_transform(df)"
      ],
      "id": "83a9fe42",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.fragment}\n",
        "- Apply the `Cleaner` to all columns\n",
        "- Split columns by dtype and # of unique values\n",
        "- Encode each column separately\n",
        ":::\n",
        "\n",
        "\n",
        "## Encoding _all the features_: `TableVectorizer` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "![](images/skrub-table-vectorizer.png)\n",
        "\n",
        "\n",
        "## Build a predictive pipeline with `tabular_pipeline` {auto-animate=\"true\" .smaller}\n",
        "```{.python}\n",
        "import skrub\n",
        "from sklearn.linear_model import Ridge\n",
        "model = skrub.tabular_pipeline(Ridge())\n",
        "```\n",
        "\n",
        "![](images/skrub-tabular-pipeline-linear-model.png){fig-align=\"center\"}\n",
        "\n",
        "# Advanced skrub: Data Ops\n",
        "\n",
        "## DataOps...\n",
        "\n",
        "::: {.incremental}\n",
        "- Extend the `scikit-learn` machinery to complex multi-table operations, and take care of data leakage\n",
        "- Track all operations with a computational graph (a *Data Ops plan*)\n",
        "- Are transparent and give direct access to the underlying object\n",
        "- Allow tuning any operation in the Data Ops plan\n",
        "- Guarantee that all operations are reproducible\n",
        "- Can be persisted and shared easily \n",
        ":::\n",
        "\n",
        "## How do DataOps work, though?  {.smaller}\n",
        "DataOps **wrap** around *user operations*, where user operations are:\n",
        "\n",
        "- any dataframe operation (e.g., merge, group by, aggregate etc.)\n",
        "- scikit-learn estimators (a Random Forest, RidgeCV etc.)\n",
        "- custom user code (load data from a path, fetch from an URL etc.)\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "::: {.callout-important}\n",
        "DataOps _record_ user operations, so that they can later be _replayed_ in the same\n",
        "order and with the same arguments on unseen data. \n",
        ":::\n",
        "::: \n",
        "\n",
        "## Starting with the `DataOps` {auto-animate=\"true\"} \n"
      ],
      "id": "5316005a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub\n",
        "data = skrub.datasets.fetch_credit_fraud()\n",
        "\n",
        "baskets = skrub.var(\"baskets\", data.baskets)\n",
        "products = skrub.var(\"products\", data.products) # add a new variable\n",
        "\n",
        "X = baskets[[\"ID\"]].skb.mark_as_X()\n",
        "y = baskets[\"fraud_flag\"].skb.mark_as_y()"
      ],
      "id": "0bf2e4b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- `baskets` and `products` represent inputs to the pipeline.\n",
        "- Skrub tracks `X` and `y` so that training and test splits are never mixed. \n",
        "\n",
        "## Applying a transformer {auto-animate=\"true\"}"
      ],
      "id": "a1766444"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# | echo: true\n",
        "from skrub import selectors as s\n",
        "\n",
        "vectorizer = skrub.TableVectorizer(\n",
        "    high_cardinality=skrub.StringEncoder()\n",
        ")\n",
        "vectorized_products = products.skb.apply(\n",
        "    vectorizer, cols=s.all() - \"basket_ID\"\n",
        ")"
      ],
      "id": "65821652",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Executing dataframe operations {auto-animate=\"true\"}"
      ],
      "id": "9a905243"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "aggregated_products = vectorized_products.groupby(\n",
        "    \"basket_ID\"\n",
        ").agg(\"mean\").reset_index()\n",
        "\n",
        "features = X.merge(\n",
        "    aggregated_products, left_on=\"ID\", right_on=\"basket_ID\"\n",
        ")\n",
        "features = features.drop(columns=[\"ID\", \"basket_ID\"])"
      ],
      "id": "94f3d3d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Applying a ML model {auto-animate=\"true\"}"
      ],
      "id": "3c188306"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from sklearn.ensemble import ExtraTreesClassifier  \n",
        "predictions = features.skb.apply(\n",
        "    ExtraTreesClassifier(n_jobs=-1), y=y\n",
        ")"
      ],
      "id": "b1dc4cab",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the Data Ops plan\n",
        "```{.python}\n",
        "predictions.skb.full_report()\n",
        "```\n",
        "<br/>\n",
        "\n",
        "<a href=\"dataop_report/index.html\" target=\"_blank\">Execution report</a>\n",
        "\n",
        "Each node:\n",
        "\n",
        "- Shows a preview of the data resulting from the operation\n",
        "- Reports the location in the code where the code is defined\n",
        "- Shows the run time of the node \n",
        "\n",
        "## Exporting the plan in a `learner` {.smaller}\n",
        "The **Learner** is a stand-alone object that works like\n",
        "a scikit-learn estimator that takes a dictionary as input rather\n",
        "than just `X` and `y`. \n",
        "\n",
        "\n",
        "::: {.fragment}\n"
      ],
      "id": "39c7ef76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "learner = predictions.skb.make_learner(fitted=True)"
      ],
      "id": "bcfc10b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "::: {.fragment}\n",
        "Then, the `learner` can be pickled ...\n"
      ],
      "id": "9e043420"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle \n",
        "\n",
        "learner_bytes = pickle.dumps(learner)"
      ],
      "id": "84524ff8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{.python}\n",
        "import pickle\n",
        "\n",
        "with open(\"learner.bin\", \"wb\") as fp:\n",
        "    pickle.dump(learner, fp)\n",
        "```"
      ],
      "id": "8ca951cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loaded_learner = pickle.loads(learner_bytes)"
      ],
      "id": "ccb7cf77",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "... loaded and applied to new data:\n",
        "\n",
        "```{.python}\n",
        "with open(\"learner.bin\", \"rb\") as fp:\n",
        "    loaded_learner = pickle.load(fp)\n",
        "data = skrub.datasets.fetch_credit_fraud(split=\"test\")\n",
        "new_baskets = data.baskets\n",
        "new_products = data.products\n",
        "loaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})\n",
        "```"
      ],
      "id": "2605ca7f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "data = skrub.datasets.fetch_credit_fraud(split=\"test\")\n",
        "new_baskets = data.baskets\n",
        "new_products = data.products\n",
        "loaded_learner.predict(\n",
        "    {\"baskets\": new_baskets, \"products\": new_products}\n",
        ")"
      ],
      "id": "4b99fc4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Hyperparameter tuning in a Data Ops plan \n",
        "\n",
        "- `choose_from`: select from the given list of options\n",
        "- `choose_int`: select an integer within a range\n",
        "- `choose_float`: select a float within a range\n",
        "- `choose_bool`: select a bool \n",
        "- `optional`: chooses whether to execute the given operation\n",
        "\n",
        "\n",
        "## Tuning in `scikit-learn` can be complex {.smaller auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "pipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\n",
        "grid = [\n",
        "    {\n",
        "        \"dim_reduction\": [PCA()],\n",
        "        \"dim_reduction__n_components\": [10, 20, 30],\n",
        "        \"regressor\": [Ridge()],\n",
        "        \"regressor__alpha\": loguniform(0.1, 10.0),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [SelectKBest()],\n",
        "        \"dim_reduction__k\": [10, 20, 30],\n",
        "        \"regressor\": [Ridge()],\n",
        "        \"regressor__alpha\": loguniform(0.1, 10.0),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [PCA()],\n",
        "        \"dim_reduction__n_components\": [10, 20, 30],\n",
        "        \"regressor\": [RandomForestRegressor()],\n",
        "        \"regressor__n_estimators\": loguniform(20, 200),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [SelectKBest()],\n",
        "        \"dim_reduction__k\": [10, 20, 30],\n",
        "        \"regressor\": [RandomForestRegressor()],\n",
        "        \"regressor__n_estimators\": loguniform(20, 200),\n",
        "    },\n",
        "]\n",
        "```\n",
        "## Tuning with Data Ops is simple! {.smaller} \n",
        "\n",
        "```python \n",
        "dim_reduction = X.skb.apply(\n",
        "    skrub.choose_from(\n",
        "        {\n",
        "            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n",
        "            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n",
        "        }, name=\"dim_reduction\"\n",
        "    )\n",
        ")\n",
        "regressor = dim_reduction.skb.apply(\n",
        "    skrub.choose_from(\n",
        "        {\n",
        "            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n",
        "            \"RandomForest\": RandomForestRegressor(\n",
        "                n_estimators=skrub.choose_int(20, 200, log=True)\n",
        "            )\n",
        "        }, name=\"regressor\"\n",
        "    )\n",
        ")\n",
        "```\n",
        "\n",
        "## Exploring the hyperparameters {auto-animate=\"true\" .smaller} \n",
        "\n",
        "```{.python}\n",
        "search = pred.skb.get_randomized_search(fitted=True)\n",
        "search.plot_parallel_coord()\n",
        "```"
      ],
      "id": "e28734f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plotly.io import read_json\n",
        "\n",
        "fig = read_json(\"parallel_coordinates_hgbr.json\")\n",
        "fig.update_layout(margin=dict(l=200))"
      ],
      "id": "c3cfbf81",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Resources and contacts\n",
        "\n",
        "## Examples and guides\n",
        "- Skrub [example gallery](https://skrub-data.org/stable/auto_examples/data_ops/index.html)\n",
        "- Skrub [user guide](https://skrub-data.org/stable/documentation.html)\n",
        "- [Tutorial](https://github.com/skrub-data/EuroSciPy2025) on timeseries \n",
        "forecasting at Euroscipy 2025\n",
        "- [Kaggle notebook](https://www.kaggle.com/code/ryye107/titanic-challenge-with-the-skrub-data-ops) \n",
        "on the Titanic survival challenge\n",
        "\n",
        "##  Getting involved {.smaller}\n",
        "::: {.nonincremental}\n",
        "Do you want to learn more? \n",
        "\n",
        "- [Skrub website](https://skrub-data.org/stable/) \n",
        "- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)\n",
        "- [Discord server](https://discord.gg/ABaPnm7fDC)\n",
        "\n",
        "Follow skrub on:\n",
        "\n",
        "- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)\n",
        "- [LinkedIn](https://www.linkedin.com/company/skrub-data/)\n",
        "\n",
        "Star skrub on GitHub, or contribute directly: \n",
        "\n",
        "- [GitHub repository](https://github.com/skrub-data/skrub/)\n",
        ":::\n",
        "\n",
        "# Contributing to skrub: open issues\n",
        "**[GitHub project](https://github.com/orgs/skrub-data/projects/9/)**\n",
        "\n",
        "## Before you start working on an issue\n",
        "\n",
        "::: {.callout-important}\n",
        "Write a comment on the issue so we know you're working on it.\n",
        ":::\n",
        "\n",
        "\n",
        "We want to avoid having multiple people working on the same issue in separate PRs. \n",
        "\n",
        "## Legend:\n",
        "\n",
        "- üò¥ : easy issue\n",
        "- üçÅ : some complexity\n",
        "- üë∫ : hard problem\n",
        "- üèÉ‚Äç‚ôÄÔ∏è : quick to solve\n",
        "- üõå : likely will take a while\n",
        "- üêà : docs, need to deal with Sphinx\n",
        "\n",
        "## `TableReport` {.smaller}\n",
        "- [#1175](https://github.com/skrub-data/skrub/issues/1175) - Better control over\n",
        "the `TableReport`'s progress display. üë∫ üõå\n",
        "- [#1523](https://github.com/skrub-data/skrub/issues/1523) - Fix the behavior of \n",
        "the `TableReport` when `max_plot_columns` is set to `None`. üçÅ üèÉ\n",
        "- [#1178](https://github.com/skrub-data/skrub/issues/1178) - Shuffle the rows of \n",
        "the `TableReport` example in the home page.  üò¥ üèÉ\n",
        "\n",
        "## New transformers  {.smaller}\n",
        "- [#1001](https://github.com/skrub-data/skrub/issues/1001) - Add a `DropSimilar`\n",
        "transformer. üë∫ üõå \n",
        "- [#710](https://github.com/skrub-data/skrub/issues/710) - Add holidays as features. üë∫ üõå\n",
        "- [#1542](https://github.com/skrub-data/skrub/issues/1542) - Add a transformer \n",
        "that parses string columns that include units (kg, $ etc). üë∫ üõå\n",
        "- [#1430](https://github.com/skrub-data/skrub/issues/1430) - Extend `ToDatetime` \n",
        "so that it can take a list of datetime formats. üçÅ üõå\n",
        "\n",
        "## Bugfixes and maintenance {.smaller}\n",
        "- [#1675](https://github.com/skrub-data/skrub/issues/1675) - Improve error message \n",
        "when the `TableReport` receives a lazy Polars dataframe.   üò¥ üèÉ\n",
        "- [#1490](https://github.com/skrub-data/skrub/issues/1490) - Cleaner fails when there \n",
        "is an empty polars column name. üë∫ üèÉ\n",
        "\n",
        "## Documentation {.smaller}\n",
        "\n",
        "- [#1476](https://github.com/skrub-data/skrub/issues/1476) - DOC: add an example \n",
        "dedicated to showing the features of the `TableReport`. üçÅ üõå üêà\n",
        "- [#991](https://github.com/skrub-data/skrub/issues/991) - Move the dev docs of\n",
        "the `TableReport` to the main documentation page. üçÅ üêà\n",
        "- [#1582](https://github.com/skrub-data/skrub/issues/1582) - Reorganize the \n",
        "\"Development\" section in the top bar. üçÅ üõå üêà\n",
        "- [#1425](https://github.com/skrub-data/skrub/issues/1425) - Shorten the note on the \n",
        "single-column transformer. üò¥ üèÉ\n",
        "- [#1660](https://github.com/skrub-data/skrub/issues/1660) - Add different doc\n",
        "versions to the switcher.  üçÅ üêà üèÉ\n",
        "- [#1616](https://github.com/skrub-data/skrub/issues/1616) - Change the numbering\n",
        "of examples. üò¥ üêà üèÉ\n",
        "\n",
        "## Examples  {.smaller}\n",
        "- [#1629](https://github.com/skrub-data/skrub/issues/1629) - Add an example for the \n",
        "`DatetimeEncoder`. üë∫ üõå üêà\n",
        "- [#1234](https://github.com/skrub-data/skrub/issues/1234) - Shuffle the toxicity\n",
        "dataset in the example. üò¥ üèÉ\n",
        "- Any example you can come up with! \n",
        "\n",
        "# Contributing to skrub: preparation\n",
        "Instructions are also available in the Installation page of the webiste:\n",
        "`https://skrub-data.org/stable/install.html`\n",
        "\n",
        "## Setting up the repository {.smaller}\n",
        "First off, you need to fork the skrub repository:\n",
        "`https://github.com/skrub-data/skrub/fork`\n",
        "\n",
        "Then, clone the repo on your local machine\n",
        "```sh\n",
        "git clone https://github.com/<YOUR_USERNAME>/skrub\n",
        "cd skrub\n",
        "\n",
        "```\n",
        "Add the `upstream` remote to pull the latest version of skrub:\n",
        "```sh\n",
        "git remote add upstream https://github.com/skrub-data/skrub.git\n",
        "```\n",
        "You can check that the remote has been added with `git remote -v`.\n",
        "\n",
        "## Setting up the environment {.smaller}\n",
        "Depends on the tools you use! \n",
        "\n",
        "From inside the skrub directory you just cloned: \n",
        "\n",
        "::: {.panel-tabset}\n",
        "### venv\n",
        "- Create the venv (in the current dir): \n",
        "```sh\n",
        "python -m venv dev-skrub\n",
        "```\n",
        "- Activate the venv: \n",
        "```sh\n",
        "source dev-skrub/bin/activate\n",
        "```\n",
        "- Install skrub and dependencies: \n",
        "```sh\n",
        "pip install -e \".[dev]\"\n",
        "```\n",
        "\n",
        "### uv \n",
        "- Create the venv (in the current dir): \n",
        "```sh\n",
        "uv venv dev-skrub \n",
        "```\n",
        "- Activate the venv:\n",
        "```sh\n",
        "source activate \n",
        "```\n",
        "- Install skrub and dev dependencies:\n",
        "```sh\n",
        "uv pip install -e \".[dev]\"\n",
        "```\n",
        "\n",
        "### conda\n",
        "- Create the conda environment:\n",
        "```sh\n",
        "conda create -n dev-skrub\n",
        "\n",
        "```\n",
        "- Activate the enviornment\n",
        "```sh\n",
        "conda activate dev-skrub\n",
        "```\n",
        "- Install skrub and dependencies: \n",
        "```sh\n",
        "pip install -e \".[dev]\"\n",
        "```\n",
        "\n",
        "\n",
        "### pixi\n",
        "- Install pixi: `https://pixi.sh/latest/installation/`\n",
        "- Go to the skrub folder\n",
        "- Install an environment:\n",
        "```sh\n",
        "pixi install dev\n",
        "# activate dev from IDE\n",
        "```\n",
        "- Run a command in a specific environment:\n",
        "```sh\n",
        "pixi run -e ci-py309-min-deps COMMAND\n",
        "```\n",
        "- Spawn a shell with the given env:\n",
        "```sh\n",
        "pixi shell -e ci-latest-optional-deps\n",
        "```\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "My recommendation: use pixi! \n",
        "\n",
        ":::\n",
        "\n",
        "## Note on the `pixi.lock` file\n",
        "\n",
        "::: {.callout-important}\n",
        "If you use pixi, it may happen that the `pixi.lock` file will be updated as you\n",
        "run commands. \n",
        "\n",
        "**Revert** changes to this file before adding files and pushing upstream. \n",
        ":::\n",
        "\n",
        "\n",
        "## Running tests {.smaller}\n",
        "::: {.panel-tabset}\n",
        "### Using environments\n",
        "From inside the root skrub folder, and after activating the environment\n",
        "```sh\n",
        "pytest --pyargs skrub\n",
        "```\n",
        "\n",
        "### With pixi\n",
        "Run all tests (you will be prompted to choose an env):\n",
        "```sh\n",
        "pixi run test\n",
        "```\n",
        "Run tests in a specific env\n",
        "```sh\n",
        "pixi run -e dev test\n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        "## Running tests\n",
        "Tests are stored in `skrub/tests`, or in a `tests` subfolder. \n",
        "\n",
        "It is possible to run specific tests by providing a path. To test `TableVectorizer`: \n",
        "\n",
        "```sh\n",
        "pytest -vsl skrub/tests/test_table_vectorizer.py \n",
        "```\n",
        "\n",
        "`-vsl` prints out more information compared to the default. \n",
        "\n",
        "## Working on the documentation {.smaller}\n",
        "Docs are written in RST and use the\n",
        "[Sphinx library](https://www.sphinx-doc.org/en/master/index.html) for rendering, \n",
        "cross-references and everything else. \n",
        "\n",
        "::: {.panel-tabset}\n",
        "### From environment\n",
        "\n",
        "Build the doc from the `doc` folder using:\n",
        "```sh\n",
        "# Build the full documentation, including examples\n",
        "make html\n",
        "\n",
        "# Build documentation without running examples (faster)\n",
        "make html-noplot\n",
        "\n",
        "# Clean previously built documentation\n",
        "make clean\n",
        "```\n",
        "\n",
        "### With pixi\n",
        "From the skrub root folder (where `pyproject.toml` is):\n",
        "```sh\n",
        "# Build the full documentation, including examples\n",
        "pixi run build-doc\n",
        "\n",
        "# Build documentation without running examples (faster)\n",
        "pixi run build-doc-quick\n",
        "\n",
        "# Clean previously built documentation\n",
        "pixi run clean-doc\n",
        "```\n",
        ":::\n",
        "\n",
        "After rendering the docs, open the `doc/_build/html/index.html` file with a \n",
        "browser.\n",
        "\n",
        "## Writing an example\n",
        "Full guide: `https://skrub-data.org/stable/tutorial_example.html`\n",
        "\n",
        "\n",
        "## Opening a PR and contributing upstream {.smaller}\n",
        "More detail is available on the main website:\n",
        "`https://skrub-data.org/stable/CONTRIBUTING.html`\n",
        "\n",
        "Start by creating a branch:\n",
        "```sh\n",
        "# fetch latest updates and start from the current head\n",
        "git fetch upstream\n",
        "git checkout -b my-branch-name-eg-fix-issue-123\n",
        "```\n",
        "\n",
        "Make some changes, then:\n",
        "```sh\n",
        "git add ./the/file-i-changed\n",
        "git commit -m \"my message\"\n",
        "git push --set-upstream origin my-branch-name-eg-fix-issue-123\n",
        "```\n",
        "At this point, visit the GitHub PR page and open a PR from there:\n",
        "`https://github.com/skrub-data/skrub/pulls`\n",
        "\n",
        "## Code formatting and `pre-commit` {.smaller}\n",
        "Formatting is enforced though pre-commit checks.\n",
        "\n",
        "::: {.panel-tabset}\n",
        "### From environment\n",
        "- Make sure `pre-commit` is installed in the environment and the folder\n",
        "```sh\n",
        "pre-commit install\n",
        "```\n",
        "- If missing, install it with `pip`\n",
        "```sh\n",
        "pip install pre-commit\n",
        "```\n",
        "Then, add modified files and run pre-commit on them. \n",
        "\n",
        "```sh\n",
        "git add YOUR_FILE\n",
        "pre-commit\n",
        "# if the file has been formatted, add again\n",
        "git add YOUR_FILE\n",
        "# commit the changes\n",
        "git commit -m \"MY COMMIT MESSAGE\"\n",
        "# pre-commit will run again automatically\n",
        "# push the  commit\n",
        "git push\n",
        "```\n",
        "\n",
        "\n",
        "### Pixi\n",
        "If you're working entirely with pixi:\n",
        "\n",
        "```sh\n",
        "git add YOUR_FILE\n",
        "pixi run lint\n",
        "# if the file has been formatted, add again\n",
        "git add YOUR_FILE\n",
        "# commit the changes\n",
        "git commit -m \"MY COMMIT MESSAGE\"\n",
        "# pre-commit will run again automatically\n",
        "# push the  commit\n",
        "git push\n",
        "```\n",
        ":::\n",
        "\n",
        "\n",
        "## Where to find this presentation\n",
        "::: {.columns}\n",
        "::: {.column}\n",
        "\n",
        "Slides\n",
        "![](images/qr-code.png)\n",
        "\n",
        ":::\n",
        "::: {.column}\n",
        "GitHub project\n",
        "![](images/qr-code-github-project.png)\n",
        "\n",
        ":::\n",
        "::: "
      ],
      "id": "dadad224"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/rcap/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}