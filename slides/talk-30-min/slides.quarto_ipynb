{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Skrub\"\n",
        "title-block-banner: true\n",
        "date: 2025-09-16\n",
        "subtitle: \"Machine learning with dataframes\"\n",
        "author: \"Riccardo Cappuzzo\"\n",
        "institute: \"Inria P16\"\n",
        "format: \n",
        "    revealjs:\n",
        "        slide-number: c/t\n",
        "        show-slide-number: all\n",
        "        preview-links: auto\n",
        "        embed-resources: false\n",
        "        transition: slide\n",
        "        theme: simple\n",
        "        logo: images/skrub.svg\n",
        "        css: style.css\n",
        "        footer: \"https://skrub-data.org/skrub-materials/\"\n",
        "incremental: false\n",
        "params: \n",
        "    version: \"base\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## `whoami`  {visibility=\"hidden\"}\n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "- I am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub ![](images/inria.png){width=250}\n",
        "\n",
        "- I'm Italian, but I don't drink coffee, wine, and I like pizza with fries ![](images/raora.png){width=50}\n",
        "\n",
        "- I did my PhD in CÃ´te d'Azur, and I moved away because it was too sunny and \n",
        "I don't like the sea ![](images/nice.jpg){width=250}\n",
        "\n",
        ":::\n",
        "\n",
        "# Boost your productivity with skrub! {auto-animate=\"true\"}\n",
        "\n",
        "Skrub simplifies many tedious data preparation operations\n",
        "\n",
        "\n",
        "## A teaser for later... {auto-animate=\"true\"}\n",
        "\n",
        "Inspect all the steps of your pipeline: \n",
        "<a href=\"dataop_report/index.html\" target=\"_blank\">Execution report</a>\n",
        "\n",
        "## A teaser for later... {auto-animate=\"true\"} \n",
        "Explore your hyperparameter search space"
      ],
      "id": "5b783a20"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plotly.io import read_json\n",
        "\n",
        "fig = read_json(\"parallel_coordinates_hgbr.json\")\n",
        "fig.update_layout(margin=dict(l=200))"
      ],
      "id": "905f1d9e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## skrub compatibility\n",
        "- skrub is fully compatible with pandas and polars\n",
        "- skrub transformers are fully compatible with scikit-learn\n",
        "\n",
        "## An example pipeline\n",
        "1. Gather some data\n",
        "2. Explore the data\n",
        "3. Preprocess the data \n",
        "4. Perform feature engineering \n",
        "5. Build a scikit-learn pipeline\n",
        "6. ???\n",
        "7. Profit?  \n",
        "\n",
        "##  \n",
        "![](images/here-we-go-again.png)\n",
        "\n",
        "## Exploring the data {.smaller auto-animate=\"true\"}\n",
        "```{.python}\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skrub\n",
        "\n",
        "dataset = skrub.datasets.fetch_employee_salaries()\n",
        "employees, salaries = dataset.X, dataset.y\n",
        "\n",
        "df = pd.DataFrame(employees)\n",
        "\n",
        "# Plot the distribution of the numerical values using a histogram\n",
        "fig, axs = plt.subplots(2,1, figsize=(10, 6))\n",
        "ax1, ax2 = axs\n",
        "\n",
        "ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\n",
        "ax1.set_xlabel('Year first hired')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Count the frequency of each category\n",
        "category_counts = df['department'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "category_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n",
        "\n",
        "# Add labels and title\n",
        "ax2.set_xlabel('Department')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n",
        "\n",
        "fig.suptitle(\"Distribution of values\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "```\n",
        "## Exploring the data {.smaller auto-animate=\"true\"}\n"
      ],
      "id": "0aff0b90"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skrub\n",
        "from skrub.datasets import fetch_employee_salaries\n",
        "from pprint import pprint\n",
        "\n",
        "dataset = fetch_employee_salaries()\n",
        "employees, salaries = dataset.X, dataset.y\n",
        "\n",
        "df = pd.DataFrame(employees)\n",
        "\n",
        "# Plot the distribution of the numerical values using a histogram\n",
        "fig, axs = plt.subplots(2,1, figsize=(10, 6))\n",
        "ax1, ax2 = axs\n",
        "\n",
        "ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\n",
        "ax1.set_xlabel('Year first hired')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Count the frequency of each category\n",
        "category_counts = df['department'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "category_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n",
        "\n",
        "# Add labels and title\n",
        "ax2.set_xlabel('Department')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n",
        "\n",
        "fig.suptitle(\"Distribution of values\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "342ab204",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring the data with `skrub` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "from skrub import TableReport\n",
        "TableReport(employee_salaries)\n",
        "```\n",
        "[Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link=\"true\"}\n",
        "\n",
        "\n",
        "::: {.fragment}\n",
        "::: {.nonincremental}\n",
        "Main features:\n",
        "\n",
        "- Obtain high-level statistics about the data\n",
        "- Explore the distribution of values and find outliers\n",
        "- Discover highly correlated columns \n",
        "- Export and share the report as an `html` file\n",
        ":::\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "<a href=\"https://skrub-data.org/skrub-reports/examples/\" target=\"_blank\">More examples</a>\n",
        "\n",
        ":::\n",
        "\n",
        "## Data cleaning with pandas/polars: setup {.smaller auto-animate=\"true\"}\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "d257bfc4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = {\n",
        "    'Constant int': [1, 1, 1],  # Single unique value\n",
        "    'B': [2, 3, 2],  # Multiple unique values\n",
        "    'Constant str': ['x', 'x', 'x'],  # Single unique value\n",
        "    'D': [4, 5, 6],  # Multiple unique values\n",
        "    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n",
        "    'All empty': ['', '', ''],  # All empty strings\n",
        "    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n",
        "}\n",
        "\n",
        "df_pd = pd.DataFrame(data)\n",
        "display(df_pd)"
      ],
      "id": "0d6046c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "a6f959f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "data = {\n",
        "    'Constant int': [1, 1, 1],  # Single unique value\n",
        "    'B': [2, 3, 2],  # Multiple unique values\n",
        "    'Constant str': ['x', 'x', 'x'],  # Single unique value\n",
        "    'D': [4, 5, 6],  # Multiple unique values\n",
        "    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n",
        "    'All empty': ['', '', ''],  # All empty strings\n",
        "    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n",
        "}\n",
        "\n",
        "df_pl = pl.DataFrame(data)\n",
        "display(df_pl)"
      ],
      "id": "674bddd8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Nulls, datetimes, constant columns with pandas/polars {.smaller auto-animate=\"true\"}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "fc64050c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Parse the datetime strings with a specific format\n",
        "df_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d/%m/%Y')\n",
        "\n",
        "# Drop columns with only a single unique value\n",
        "df_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) > 1]\n",
        "\n",
        "# Function to drop columns with only missing values or empty strings\n",
        "def drop_empty_columns(df):\n",
        "    # Drop columns with only missing values\n",
        "    df_cleaned = df.dropna(axis=1, how='all')\n",
        "    # Drop columns with only empty strings\n",
        "    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n",
        "    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n",
        "    return df_cleaned\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df_pd_cleaned = drop_empty_columns(df_pd_cleaned)"
      ],
      "id": "6a7a4ab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "ddecb092"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true \n",
        "# Parse the datetime strings with a specific format\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.col(\"Date\").str.strptime(pl.Date, \"%d/%m/%Y\", strict=False).alias(\"Date\")\n",
        "])\n",
        "\n",
        "# Drop columns with only a single unique value\n",
        "df_pl_cleaned = df_pl.select([\n",
        "    col for col in df_pl.columns if df_pl[col].n_unique() > 1\n",
        "])\n",
        "\n",
        "# Import selectors for dtype selection\n",
        "import polars.selectors as cs\n",
        "\n",
        "# Drop columns with only missing values or only empty strings\n",
        "def drop_empty_columns(df):\n",
        "    all_nan = df.select(\n",
        "        [\n",
        "            col for col in df.select(cs.numeric()).columns if \n",
        "            df [col].is_nan().all()\n",
        "        ]\n",
        "    ).columns\n",
        "    \n",
        "    all_empty = df.select(\n",
        "        [\n",
        "            col for col in df.select(cs.string()).columns if \n",
        "            (df[col].str.strip_chars().str.len_chars()==0).all()\n",
        "        ]\n",
        "    ).columns\n",
        "\n",
        "    to_drop = all_nan + all_empty\n",
        "\n",
        "    return df.drop(to_drop)\n",
        "\n",
        "df_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
      ],
      "id": "5a441b33",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Data cleaning with `skrub.Cleaner` {.smaller auto-animate=\"true\"}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "3274b20c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pd)\n",
        "display(df_cleaned)"
      ],
      "id": "b04593e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "fabcf7b9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pl)\n",
        "display(df_cleaned)"
      ],
      "id": "843d2f3b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Comparison\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas\n",
        "::: {.columns}\n",
        "::: {.column}"
      ],
      "id": "77331952"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pd)"
      ],
      "id": "466ae140",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "print(df_pd_cleaned)"
      ],
      "id": "22640b5b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column}"
      ],
      "id": "4b19f9eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "print(df_cleaned)"
      ],
      "id": "2a694d9f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: \n",
        "\n",
        "### Polars\n",
        "::: {.columns}\n",
        "::: {.column}"
      ],
      "id": "07912a2a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "print(df_pl_cleaned)"
      ],
      "id": "d6647824",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: {.column}"
      ],
      "id": "12f19170"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pl)"
      ],
      "id": "939bc1c4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "print(df_cleaned)"
      ],
      "id": "a44f404f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "::: \n",
        "<!-- end columns -->\n",
        ":::\n",
        "\n",
        "## Encoding datetime features with pandas/polars {.smaller}\n",
        ":::{.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "63483905"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df_pd = pd.DataFrame(data)\n",
        "datetime_column = \"date\"\n",
        "df_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n",
        "\n",
        "df_pd['year'] = df_pd[datetime_column].dt.year\n",
        "df_pd['month'] = df_pd[datetime_column].dt.month\n",
        "df_pd['day'] = df_pd[datetime_column].dt.day\n",
        "df_pd['hour'] = df_pd[datetime_column].dt.hour\n",
        "df_pd['minute'] = df_pd[datetime_column].dt.minute\n",
        "df_pd['second'] = df_pd[datetime_column].dt.second"
      ],
      "id": "01b820bb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "55b06267"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "import polars as pl\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df_pl = pl.DataFrame(data)\n",
        "df_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n",
        "\n",
        "df_pl = df_pl.with_columns(\n",
        "    year=pl.col(\"date\").dt.year(),\n",
        "    month=pl.col(\"date\").dt.month(),\n",
        "    day=pl.col(\"date\").dt.day(),\n",
        "    hour=pl.col(\"date\").dt.hour(),\n",
        "    minute=pl.col(\"date\").dt.minute(),\n",
        "    second=pl.col(\"date\").dt.second(),\n",
        ")"
      ],
      "id": "1c194263",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Adding periodic features with pandas/polars{.smaller}\n",
        ":::{.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "f7fb1996"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}"
      ],
      "id": "c7bc11b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df_pd['hour_sin'] = np.sin(2 * np.pi * df_pd['hour'] / 24)\n",
        "df_pd['hour_cos'] = np.cos(2 * np.pi * df_pd['hour'] / 24)\n",
        "\n",
        "df_pd['month_sin'] = np.sin(2 * np.pi * df_pd['month'] / 12)\n",
        "df_pd['month_cos'] = np.cos(2 * np.pi * df_pd['month'] / 12)"
      ],
      "id": "12db78c7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "9b483c9c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df_pl = df_pl.with_columns(\n",
        "    hour_sin = np.sin(2 * np.pi * pl.col(\"hour\") / 24),\n",
        "    hour_cos = np.cos(2 * np.pi * pl.col(\"hour\") / 24),\n",
        "    \n",
        "    month_sin = np.sin(2 * np.pi * pl.col(\"month\") / 12),\n",
        "    month_cos = np.cos(2 * np.pi * pl.col(\"month\") / 12),\n",
        ")"
      ],
      "id": "ec8c6b93",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Encoding datetime features `skrub.DatetimeEncoder` {auto-animate=\"true\" visibility=\"uncounted\" .smaller}"
      ],
      "id": "85178cd9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df = pl.DataFrame(data)"
      ],
      "id": "df61356a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import DatetimeEncoder, ToDatetime\n",
        "\n",
        "X_date = ToDatetime().fit_transform(df[\"date\"])\n",
        "de = DatetimeEncoder(periodic_encoding=\"circular\")\n",
        "X_enc = de.fit_transform(X_date)\n",
        "print(X_enc)"
      ],
      "id": "f072661c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What periodic features look like\n",
        "![](images/periodic_features.png){fig-align=\"center\"}\n",
        "\n",
        "## Encoding numerical features with `skrub.SquashingScaler`"
      ],
      "id": "736c8e24"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)  # for reproducibility\n",
        "\n",
        "values = np.random.rand(100, 1)\n",
        "n_outliers = 15\n",
        "outlier_indices = np.random.choice(values.shape[0], size=n_outliers, replace=False)\n",
        "values[outlier_indices] = np.random.rand(n_outliers, 1) * 100 - 50\n",
        "\n",
        "x = np.arange(values.shape[0])\n",
        "fig, axs = plt.subplots(1, layout=\"constrained\", figsize=(6, 4))\n",
        "\n",
        "axs.plot(x,values)\n",
        "_ = axs.set(\n",
        "    title=\"Feature with outliers\",\n",
        "    ylabel=\"value\",\n",
        "    xlabel=\"Sample ID\"\n",
        "    )\n",
        "axs.axhspan(-2, 2, color=\"gray\", alpha=0.15)\n",
        "\n",
        "x_data, y_data = [30, 2]\n",
        "desc = \"Data is mostly\\nin [-2, 2]\"\n",
        "axs.annotate(\n",
        "    desc,\n",
        "    xy=(x_data, y_data),\n",
        "    xytext=(0.15, 0.8),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        "\n",
        ")\n",
        "\n",
        "x_outlier, y_outlier = np.argmax(values), np.max(values)\n",
        "desc = \"There are large\\noutliers throughout.\"\n",
        "_ = axs.annotate(\n",
        "    desc,\n",
        "    xy=(x_outlier, y_outlier),\n",
        "    xytext=(0.6, 0.85),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        "\n",
        ")"
      ],
      "id": "66e1fe07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.notes}\n",
        "Skrub wants to solve ML problems based partly on solid engineering and partly on\n",
        "statistical notions. The SquashingScaler is based on the second part, and is taken\n",
        "from a recent paper that evaluates different techniques for improving the performance\n",
        "of NNs. \n",
        ":::\n",
        "\n",
        "\n",
        "\n",
        "## Encoding numerical features with `skrub.SquashingScaler`"
      ],
      "id": "25c911a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from sklearn.preprocessing import QuantileTransformer, RobustScaler, StandardScaler\n",
        "from skrub import SquashingScaler\n",
        "\n",
        "squash_scaler = SquashingScaler()\n",
        "squash_scaled = squash_scaler.fit_transform(values)\n",
        "\n",
        "robust_scaler = RobustScaler()\n",
        "robust_scaled = robust_scaler.fit_transform(values)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "standard_scaled = standard_scaler.fit_transform(values)\n",
        "\n",
        "quantile_transformer = QuantileTransformer(n_quantiles=100)\n",
        "quantile_scaled = quantile_transformer.fit_transform(values)\n",
        "# %%\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(values.shape[0])\n",
        "fig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=(8, 5))\n",
        "\n",
        "ax = axs[0]\n",
        "ax.plot(x, sorted(values), label=\"Original Values\", linewidth=2.5)\n",
        "ax.plot(x, sorted(squash_scaled), label=\"SquashingScaler\")\n",
        "ax.plot(x, sorted(robust_scaled), label=\"RobustScaler\", linestyle=\"--\")\n",
        "ax.plot(x, sorted(standard_scaled), label=\"StandardScaler\")\n",
        "ax.plot(x, sorted(quantile_scaled), label=\"QuantileTransformer\")\n",
        "\n",
        "# Add a horizontal band in [-4, +4]\n",
        "ax.axhspan(-4, 4, color=\"gray\", alpha=0.15)\n",
        "ax.set(title=\"Original data\", xlim=[0, values.shape[0]], xlabel=\"Percentile\")\n",
        "ax.legend()\n",
        "\n",
        "ax = axs[1]\n",
        "ax.plot(x, sorted(values), label=\"Original Values\", linewidth=2.5)\n",
        "ax.plot(x, sorted(squash_scaled), label=\"SquashingScaler\")\n",
        "ax.plot(x, sorted(robust_scaled), label=\"RobustScaler\", linestyle=\"--\")\n",
        "ax.plot(x, sorted(standard_scaled), label=\"StandardScaler\")\n",
        "ax.plot(x, sorted(quantile_scaled), label=\"QuantileTransformer\")\n",
        "\n",
        "ax.set(ylim=[-4, 4])\n",
        "ax.set(title=\"In range [-4, 4]\", xlim=[0, values.shape[0]], xlabel=\"Percentile\")\n",
        "\n",
        "# Highlight the bounds of the SquashingScaler\n",
        "ax.axhline(y=3, alpha=0.2)\n",
        "ax.axhline(y=-3, alpha=0.2)\n",
        "\n",
        "fig.suptitle(\n",
        "    \"Comparison of different scalers on sorted data with outliers\", fontsize=20\n",
        ")\n",
        "fig.supylabel(\"Value\")\n",
        "\n",
        "desc = \"The RobustScaler is\\naffected by outliers\"\n",
        "axs[0].annotate(\n",
        "    desc,\n",
        "    xy=(0, -70),\n",
        "    xytext=(0.4, 0.2),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        ")\n",
        "\n",
        "desc = \"The SquashingScaler is\\nclipped to a finite value\"\n",
        "_ = axs[1].annotate(\n",
        "    desc,\n",
        "    xy=(0, -3),\n",
        "    xytext=(0.4, 0.2),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        ")"
      ],
      "id": "293cfc9c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding categorical (string/text) features\n",
        "Categorical features have a \"cardinality\": the number of unique values\n",
        "\n",
        "- Low cardinality features: `OneHotEncoder`\n",
        "- High cardinality features (>40 unique values): `skrub.StringEncoder`\n",
        "- Textual features: `skrub.TextEncoder` and pretrained models from HuggingFace Hub\n",
        "\n",
        "## Encoding _all the features_: `TableVectorizer` { auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "from skrub import TableVectorizer, TextEncoder\n",
        "\n",
        "text = TextEncoder()\n",
        "table_vec = TableVectorizer(high_cardinality=text)\n",
        "df_encoded = table_vec.fit_transform(df)\n",
        "\n",
        "```\n",
        "\n",
        "## Encoding _all the features_: `TableVectorizer` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "![](images/skrub-table-vectorizer.png)\n",
        "\n",
        "\n",
        "## Fine-grained column transformations with `ApplyToCols` {.smaller auto-animate=\"true\"}\n"
      ],
      "id": "35d49a47"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import pandas as pd\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "df = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n",
        "\n",
        "categorical_columns = selector(dtype_include=object)(df)\n",
        "numerical_columns = selector(dtype_exclude=object)(df)\n",
        "\n",
        "ct = make_column_transformer(\n",
        "      (StandardScaler(),\n",
        "       numerical_columns),\n",
        "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
        "       categorical_columns))\n",
        "transformed = ct.fit_transform(df)\n",
        "transformed"
      ],
      "id": "fc7f4542",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fine-grained column transformations with `ApplyToCols` {.smaller auto-animate=\"true\"}"
      ],
      "id": "dc55371e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub.selectors as s\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from skrub import ApplyToCols\n",
        "\n",
        "numeric = ApplyToCols(StandardScaler(), cols=s.numeric())\n",
        "string = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n",
        "\n",
        "transformed = make_pipeline(numeric, string).fit_transform(df)\n",
        "transformed"
      ],
      "id": "1e367fa8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Build a predictive pipeline {auto-animate=\"true\"}\n",
        "```{.python}\n",
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge()\n",
        "```\n",
        "\n",
        "## Build a predictive pipeline {auto-animate=\"true\" visibility=\"uncounted\"}\n",
        "```{.python code-line-numbers=\"3-6\"}\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "model = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())\n",
        "```\n",
        "\n",
        "\n",
        "## Build a predictive pipeline {auto-animate=\"true\" visibility=\"uncounted\"}\n",
        "```{.python code-line-numbers=\"3,5,6-17|\"}\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "categorical_columns = selector(dtype_include=object)(employees)\n",
        "numerical_columns = selector(dtype_exclude=object)(employees)\n",
        "\n",
        "ct = make_column_transformer(\n",
        "      (StandardScaler(),\n",
        "       numerical_columns),\n",
        "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
        "       categorical_columns))\n",
        "\n",
        "model = make_pipeline(ct, SimpleImputer(), Ridge())\n",
        "```\n",
        "## Build a predictive pipeline with `tabular_pipeline` {auto-animate=\"true\" .smaller}"
      ],
      "id": "64766b1d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub\n",
        "from sklearn.linear_model import Ridge\n",
        "model = skrub.tabular_pipeline(Ridge())"
      ],
      "id": "26cf14c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](images/skrub-tabular-pipeline-linear-model.png){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "![](images/drakeno.png){fig-align=\"center\"}\n",
        "\n",
        "## We now have a pipeline! {.smaller}\n",
        "\n",
        "1. Gather some data\n",
        "2. Explore the data\n",
        "    - `skrub.TableReport`\n",
        "3. Pre-process the data \n",
        "    - `Cleaner`, `ToDatetime` ... \n",
        "4. Perform feature engineering\n",
        "    - `skrub.TableVectorizer`,`SquashingScaler`, `TextEncoder`, `StringEncoder `...\n",
        "5. Build a scikit-learn pipeline\n",
        "    - `tabular_pipeline`, `sklearn.pipeline.make_pipeline` ... \n",
        "6. ???\n",
        "7. Profit ðŸ“ˆ \n",
        "\n",
        "\n",
        "# What if this is not enough?? \n",
        "\n",
        "## What if...\n",
        "\n",
        "- Your data is spread over multiple tables? \n",
        "- You want to avoid data leakage? \n",
        "- You want to tune more than just the hyperparameters of your model? \n",
        "- You want to guarantee that your pipeline is replayed exactly on new data? \n",
        "\n",
        "## \n",
        "When a normal pipe is not enough...\n",
        "\n",
        "::: {.fragment style=\"font-size:2em;\"}\n",
        "... the `skrub` DataOps come to the rescue ðŸš’\n",
        ":::\n",
        "\n",
        "\n",
        "## DataOps...\n",
        "- Extend the `scikit-learn` machinery to complex multi-table operations, and take care of data leakage\n",
        "- Track all operations with a computational graph (a *Data Ops plan*)\n",
        "- Allow tuning any operation in the data plan\n",
        "- Can be persisted and shared easily \n",
        "\n",
        "## How do DataOps work, though?  {.smaller}\n",
        "DataOps **wrap** around *user operations*, where user operations are:\n",
        "\n",
        "- any dataframe operation (e.g., merge, group by, aggregate etc.)\n",
        "- scikit-learn estimators (a Random Forest, RidgeCV etc.)\n",
        "- custom user code (load data from a path, fetch from an URL etc.)\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "::: {.callout-important}\n",
        "DataOps _record_ user operations, so that they can later be _replayed_ in the same\n",
        "order and with the same arguments on unseen data. \n",
        ":::\n",
        "::: \n",
        "\n",
        "## DataOps, Plans, `learner`s: oh my!  \n",
        "- A `DataOp` (singular) wraps a single operation, and can be combined and concatenated with other `DataOps`. \n",
        "\n",
        "- The **Data Ops** Plan is a collective name for the directed computational graph\n",
        "that tracks a sequence and combination of `DataOps`. \n",
        "\n",
        "- The plan can be exported as a standalone object called `learner`. The `learner` \n",
        "works like a scikit-learn estimator that takes a dictionary of values rather \n",
        "than just `X` and `y`. \n",
        "\n",
        "## Starting with the `DataOps`\n"
      ],
      "id": "f761001f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub\n",
        "data = skrub.datasets.fetch_credit_fraud()\n",
        "\n",
        "baskets = skrub.var(\"baskets\", data.baskets)\n",
        "products = skrub.var(\"products\", data.products) # add a new variable\n",
        "\n",
        "X = baskets[[\"ID\"]].skb.mark_as_X()\n",
        "y = baskets[\"fraud_flag\"].skb.mark_as_y()"
      ],
      "id": "0ecd35e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.incremental}\n",
        "- `X`, `y`, `products` represent inputs to the pipeline.\n",
        "- `skrub` splits `X` and `y` when training. \n",
        ":::\n",
        "\n",
        "##  Building a full data plan"
      ],
      "id": "24c88909"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import selectors as s\n",
        "\n",
        "vectorizer = skrub.TableVectorizer(\n",
        "    high_cardinality=skrub.StringEncoder()\n",
        ")\n",
        "vectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")"
      ],
      "id": "4e10b0a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Building a full data plan {auto-animate=\"true\"}"
      ],
      "id": "930e00b6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "aggregated_products = vectorized_products.groupby(\n",
        "    \"basket_ID\"\n",
        ").agg(\"mean\").reset_index()\n",
        "\n",
        "features = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\n",
        "features = features.drop(columns=[\"ID\", \"basket_ID\"])"
      ],
      "id": "58030e27",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Building a full data plan {auto-animate=\"true\"}"
      ],
      "id": "779dcb52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from sklearn.ensemble import ExtraTreesClassifier  \n",
        "predictions = features.skb.apply(\n",
        "    ExtraTreesClassifier(n_jobs=-1), y=y\n",
        ")"
      ],
      "id": "7663b936",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the data plan\n",
        "```{.python}\n",
        "predictions.skb.full_report()\n",
        "```\n",
        "<br/>\n",
        "\n",
        "<a href=\"dataop_report/index.html\" target=\"_blank\">Execution report</a>\n",
        "\n",
        "Each node:\n",
        "\n",
        "- Shows a preview of the data resulting from the operation\n",
        "- Reports the location in the code where the code is defined\n",
        "- Shows the run time of the node (in the next release)\n",
        "\n",
        "## Exporting the plan in a `learner` {.smaller}\n",
        "The data plan can be exported as a `learner`:"
      ],
      "id": "ecce5540"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# anywhere\n",
        "learner = predictions.skb.make_learner(fitted=True)"
      ],
      "id": "1dc6d94f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.fragment}\n",
        "Then, the `learner` can be pickled ...\n"
      ],
      "id": "3bc5ab76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle \n",
        "\n",
        "learner_bytes = pickle.dumps(learner)"
      ],
      "id": "5d4b32a8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{.python}\n",
        "import pickle\n",
        "\n",
        "with open(\"learner.bin\", \"wb\") as fp:\n",
        "    pickle.dump(learner, fp)\n",
        "```\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "... loaded ...\n"
      ],
      "id": "49e29635"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loaded_learner = pickle.loads(learner_bytes)"
      ],
      "id": "8fe2745f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{.python}\n",
        "with open(\"learner.bin\", \"rb\") as fp:\n",
        "    loaded_learner = pickle.load(fp)\n",
        "```\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "... and applied to new data:"
      ],
      "id": "d6ddeb23"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "data = skrub.datasets.fetch_credit_fraud(split=\"test\")\n",
        "new_baskets = data.baskets\n",
        "new_products = data.products\n",
        "loaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})"
      ],
      "id": "e47c8237",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Hyperparameter tuning in a Data Plan \n",
        "`skrub` implements four `choose_*` functions:\n",
        "\n",
        "- `choose_from`: select from the given list of options\n",
        "- `choose_int`: select an integer within a range\n",
        "- `choose_float`: select a float within a range\n",
        "- `choose_bool`: select a bool \n",
        "- `optional`: chooses whether to execute the given operation\n",
        "\n",
        "\n",
        "## Tuning in `scikit-learn` can be complex {.smaller auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "pipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\n",
        "grid = [\n",
        "    {\n",
        "        \"dim_reduction\": [PCA()],\n",
        "        \"dim_reduction__n_components\": [10, 20, 30],\n",
        "        \"regressor\": [Ridge()],\n",
        "        \"regressor__alpha\": loguniform(0.1, 10.0),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [SelectKBest()],\n",
        "        \"dim_reduction__k\": [10, 20, 30],\n",
        "        \"regressor\": [Ridge()],\n",
        "        \"regressor__alpha\": loguniform(0.1, 10.0),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [PCA()],\n",
        "        \"dim_reduction__n_components\": [10, 20, 30],\n",
        "        \"regressor\": [RandomForestClassifier()],\n",
        "        \"regressor__n_estimators\": loguniform(20, 200),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [SelectKBest()],\n",
        "        \"dim_reduction__k\": [10, 20, 30],\n",
        "        \"regressor\": [RandomForestClassifier()],\n",
        "        \"regressor__n_estimators\": loguniform(20, 200),\n",
        "    },\n",
        "]\n",
        "model = RandomizedSearchCV(pipe, grid)\n",
        "```\n",
        "## Tuning with `DataOps` is simple! {.smaller} \n",
        "\n",
        "```python\n",
        "dim_reduction = X.skb.apply(\n",
        "    skrub.choose_from(\n",
        "        {\n",
        "            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n",
        "            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n",
        "        }, name=\"dim_reduction\"\n",
        "    )\n",
        ")\n",
        "regressor = dim_reduction.skb.apply(\n",
        "    skrub.choose_from(\n",
        "        {\n",
        "            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n",
        "            \"RandomForest\": RandomForestClassifier(\n",
        "                n_estimators=skrub.choose_int(20, 200, log=True)\n",
        "            )\n",
        "        }, name=\"regressor\"\n",
        "    )\n",
        ")\n",
        "search = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True)\n",
        "```\n",
        "\n",
        "## Tuning with `DataOps` is not limited to estimators\n",
        "::: {.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "1452008a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import skrub"
      ],
      "id": "a35172b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df = pd.DataFrame(\n",
        "    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n",
        ")\n",
        "\n",
        "df_do = skrub.var(\"grades\", df)\n",
        "\n",
        "agg_grades = df_do.groupby(\"subject\").agg(skrub.choose_from([\"count\", \"mean\"]))\n",
        "agg_grades.skb.describe_param_grid()"
      ],
      "id": "2e33ea7b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "8f25e332"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import skrub"
      ],
      "id": "26609ec6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df = pl.DataFrame(\n",
        "    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n",
        ")\n",
        "\n",
        "df_do = skrub.var(\"grades\", df)\n",
        "\n",
        "agg_grades = df_do.group_by(\"subject\").agg(\n",
        "    skrub.choose_from([pl.mean(\"grade\"), pl.count(\"grade\")])\n",
        ")\n",
        "agg_grades.skb.describe_param_grid()"
      ],
      "id": "d195aa1c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Run hyperparameter search\n",
        "```{.python}\n",
        "# fit the search \n",
        "search = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True, cv=5)\n",
        "\n",
        "# save the best learner\n",
        "best_learner = search.best_learner_\n",
        "```\n",
        "\n",
        "## Observe the impact of the hyperparameters {auto-animate=\"true\" .smaller} \n",
        "Data Ops provide a built-in parallel coordinate plot. \n",
        "\n",
        "```{.python}\n",
        "search = pred.skb.get_randomized_search(fitted=True)\n",
        "search.plot_parallel_coord()\n",
        "```"
      ],
      "id": "06a0c5aa"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plotly.io import read_json\n",
        "\n",
        "fig = read_json(\"parallel_coordinates_hgbr.json\")\n",
        "fig.update_layout(margin=dict(l=200))"
      ],
      "id": "5ab7d83b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[source](https://skrub-data.org/EuroSciPy2025/content/notebooks/single_horizon_prediction.html)\n",
        "\n",
        "## More information about the Data Ops \n",
        "- Skrub [example gallery](https://skrub-data.org/stable/auto_examples/data_ops/index.html)\n",
        "- [Tutorial](https://github.com/skrub-data/EuroSciPy2025) on timeseries \n",
        "forecasting at Euroscipy 2025\n",
        "- Skrub [User guide](https://skrub-data.org/stable/documentation.html)\n",
        "- A [Kaggle notebook](https://www.kaggle.com/code/ryye107/titanic-challenge-with-the-skrub-data-ops) \n",
        "on addressing the Titanic survival challenge with Data Ops\n",
        "\n",
        "# Wrapping up\n",
        "\n",
        "## {auto-animate=\"true\" } \n",
        "![](images/powerpuff_girls_1.png)\n",
        "\n",
        "## {auto-animate=\"true\" } \n",
        "![](images/powerpuff_girls_2.png)\n",
        "\n",
        "##  Getting involved {.smaller}\n",
        "::: {.nonincremental}\n",
        "Do you want to learn more? \n",
        "\n",
        "- [Skrub website](https://skrub-data.org/stable/) \n",
        "- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)\n",
        "- [Discord server](https://discord.gg/ABaPnm7fDC)\n",
        "\n",
        "Follow skrub on:\n",
        "\n",
        "- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)\n",
        "- [LinkedIn](https://www.linkedin.com/company/skrub-data/)\n",
        "\n",
        "Star skrub on GitHub, or contribute directly: \n",
        "\n",
        "- [Git repository](https://github.com/skrub-data/skrub/)\n",
        ":::\n",
        "\n",
        "## tl;dw\n",
        "`skrub` provides\n",
        "\n",
        "::: {.nonincremental}\n",
        "- interactive data exploration\n",
        "- automated pre-processing of pandas and polars dataframes\n",
        "- powerful feature engineering\n",
        "- DataOps, plans, hyperparameter tuning, (almost) no leakage \n",
        ":::"
      ],
      "id": "e475a91c"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/rcap/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}