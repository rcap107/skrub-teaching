---
title: "Skrub"
title-block-banner: true
date: 2025-11-21
subtitle: "Machine learning with dataframes"
author: "Riccardo Cappuzzo"
institute: "Inria P16"
format: 
    revealjs:
        slide-number: c/t
        show-slide-number: all
        preview-links: auto
        embed-resources: false
        transition: slide
        theme: simple
        logo: images/skrub.svg
        css: style.css
        footer: "https://skrub-data.org/skrub-materials/"
incremental: false
params: 
    version: "base"
---


## `whoami` 

::: {.incremental}

- I am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub ![](images/inria.png){width=250}

- I'm Italian, but I don't drink coffee, wine, and I like pizza with fries ![](images/raora.png){width=50}

- I did my PhD in CÃ´te d'Azur, and I moved away because it was too sunny and 
I don't like the sea ![](images/nice.jpg){width=250}

:::

# Building a predictive pipeline with skrub {auto-animate="true"}

## Premise and warning
::: {.callout-important}
This lecture should give you an idea of possible problems you might run into, and 
how skrub can help you deal with (some of) them. The idea is giving you material
you can refer back to if you encounter the same issue later on. 
:::


{{< include /includes/talk-sections/_skrub_compatibility.md >}}
{{< include /includes/talk-sections/_example_pipeline.md >}}
{{< include /includes/preparation/_exploring_base.md >}}
{{< include /includes/preparation/_table_report.md >}}


## Exploring the data with `skrub` {.smaller auto-animate="true"}

Things to notice

- Shape of the dataframe (rows, columns)
- Dtype of the columns (string, numerical, datetime)
    - Were dtypes converted properly? 
- Missing values
    - Are there columns with many missing values? 
- Distribution of values
    - Are there columns with high cardiinality? 
    - Are there outliers? 
    - Are there columns with imbalanced distributions? 
- Column associations
    - Are there correlated columns? 

## Intermission: reading files, parsing dtypes
- CSV vs Parquet+
- Fun with separators
-

## What happens if pandas parses this? {.smaller auto-animate="true"}
```{python}
import os

project_root = os.environ.get("QUARTO_PROJECT_DIR", ".")
file_path = os.path.join(project_root, "resources", "csv-with-comma.csv")

with open(file_path) as fp:
    for line in fp.readlines():
        print(line, end="")

```


## What happens if pandas parses this? {.smaller auto-animate="true"}
```{python}
#| echo: true

import pandas as pd
pd.read_csv(file_path)
```


## How about now? {.smaller auto-animate="true"}

```{python}
import os

project_root = os.environ.get("QUARTO_PROJECT_DIR", ".")
file_path = os.path.join(project_root, "resources", "csv-without-comma.csv")

with open(file_path) as fp:
    for line in fp.readlines():
        print(line, end="")
```

## How about now? {.smaller auto-animate="true"}
```{python}
#| echo: true

import pandas as pd
pd.read_csv(file_path)
```


{{< include /includes/preparation/_data_cleaning_base.md >}}
{{< include /includes/preparation/_data_cleaning_skrub.md >}}
{{< include /includes/encoders/_datetimes_base.md >}}

## What happens if we train directly on a datetime? 

{{< include /includes/encoders/_periodic_features_base.md >}}
{{< include /includes/encoders/_datetimes_skrub.md >}}
{{< include /includes/encoders/_periodic_features_skrub.md >}}
{{< include /includes/encoders/_squashing_scaler.md >}}



## Encoding categorical (string/text) features {.smaller}
Categorical features have a "cardinality": the number of unique values

- Low cardinality features: `OneHotEncoder`
    - The `OneHotEncoder` produces **sparse** matrices. Dataframes are **dense**.
- High cardinality features (>40 unique values): 
    - Using `OneHotEncoder` generates too many (dense) features.
    - Use encoders with a fixed number of output features.
- [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) with: `skrub.StringEncoder`
    - Apply tf-idf to the ngrams in the column, followed by SVD. 
    - Robust, quick, fixed number of output features regardless of # of unique values. 


## Encoding categorical (string/text) features {.smaller}

- Textual features: `skrub.TextEncoder` and pretrained models from [HuggingFace Hub](https://huggingface.co/models).
    - The `TextEncoder` needs Torch: _very heavy dependency_. 
    - Models are large and take time to download. 
    - Encoding is much slower.
    - However, performance can be much better depending on the dataset. 

Deeper dive in this [post](https://skrub-data.org/skrub-materials/pages/notebooks/categorical-encoders/categorical-encoders.html)

{{< include /includes/encoders/_table_vectorizer.md >}}
{{< include /includes/encoders/_apply_to.md >}}


## Selecting columns with the `skrub.selectors` {.smaller}
- 


## We now have a pipeline! {.smaller}

1. Gather some data
2. Explore the data
    - `skrub.TableReport`
3. Pre-process the data 
    - `Cleaner`, `ToDatetime` ... 
4. Perform feature engineering
    - `skrub.TableVectorizer`,`SquashingScaler`, `TextEncoder`, `StringEncoder `...
5. Build a scikit-learn pipeline
    - `tabular_pipeline`, `sklearn.pipeline.make_pipeline` ... 
6. ???
7. Profit ðŸ“ˆ 

## More information about the Data Ops 
- Skrub [example gallery](https://skrub-data.org/stable/auto_examples/data_ops/index.html)
- [Tutorial](https://github.com/skrub-data/EuroSciPy2025) on timeseries 
forecasting at Euroscipy 2025
- Skrub [User guide](https://skrub-data.org/stable/documentation.html)
- A [Kaggle notebook](https://www.kaggle.com/code/ryye107/titanic-challenge-with-the-skrub-data-ops) 
on addressing the Titanic survival challenge with Data Ops

# Wrapping up

## {auto-animate="true" } 
![](images/powerpuff_girls_1.png)

## {auto-animate="true" } 
![](images/powerpuff_girls_2.png)

##  Getting involved {.smaller}
::: {.nonincremental}
Do you want to learn more? 

- [Skrub website](https://skrub-data.org/stable/) 
- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)
- [Discord server](https://discord.gg/ABaPnm7fDC)

Follow skrub on:

- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)
- [LinkedIn](https://www.linkedin.com/company/skrub-data/)

Star skrub on GitHub, or contribute directly: 

- [Git repository](https://github.com/skrub-data/skrub/)
:::

## tl;dw
`skrub` provides

::: {.nonincremental}
- interactive data exploration
- automated pre-processing of pandas and polars dataframes
- powerful feature engineering
- DataOps, plans, hyperparameter tuning, (almost) no leakage 
:::