{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Skrub\"\n",
        "title-block-banner: true\n",
        "date: 2025-11-21\n",
        "subtitle: \"Machine learning with dataframes\"\n",
        "author: \"Riccardo Cappuzzo\"\n",
        "institute: \"Inria P16\"\n",
        "format: \n",
        "    revealjs:\n",
        "        slide-number: c/t\n",
        "        show-slide-number: all\n",
        "        preview-links: auto\n",
        "        embed-resources: false\n",
        "        transition: slide\n",
        "        theme: simple\n",
        "        logo: images/skrub.svg\n",
        "        css: style.css\n",
        "        footer: \"https://skrub-data.org/skrub-materials/\"\n",
        "incremental: false\n",
        "params: \n",
        "    version: \"base\"\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "## `whoami` \n",
        "\n",
        "::: {.incremental}\n",
        "\n",
        "- I am a research engineer at Inria as part of the P16 project, and I am the lead developer of skrub ![](images/inria.png){width=250}\n",
        "\n",
        "- I'm Italian, but I don't drink coffee, wine, and I like pizza with fries ![](images/raora.png){width=50}\n",
        "\n",
        "- I did my PhD in CÃ´te d'Azur, and I moved away because it was too sunny and \n",
        "I don't like the sea ![](images/nice.jpg){width=250}\n",
        "\n",
        ":::\n",
        "\n",
        "# Building a predictive pipeline with skrub {auto-animate=\"true\"}\n",
        "\n",
        "## Premise and warning\n",
        "::: {.callout-important}\n",
        "This lecture should give you an idea of possible problems you might run into, and \n",
        "how skrub can help you deal with (some of) them. The idea is giving you material\n",
        "you can refer back to if you encounter the same issue later on. \n",
        ":::\n",
        "\n",
        "## Skrub compatibility\n",
        "- Skrub is fully compatible with [pandas](https://pandas.pydata.org/) and [polars](https://pola.rs)\n",
        "- Skrub transformers are fully compatible with [scikit-learn](https://scikit-learn.org/stable/index.html)\n",
        "\n",
        "## An example pipeline\n",
        "1. Gather some data\n",
        "2. Explore the data\n",
        "3. Preprocess the data \n",
        "4. Perform feature engineering \n",
        "5. Build a scikit-learn pipeline\n",
        "6. ???\n",
        "7. Profit?  \n",
        "\n",
        "##  \n",
        "![](images/here-we-go-again.png)\n",
        "\n",
        "## Exploring the data {.smaller auto-animate=\"true\"}\n",
        "```{.python}\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skrub\n",
        "\n",
        "dataset = skrub.datasets.fetch_employee_salaries()\n",
        "employees, salaries = dataset.X, dataset.y\n",
        "\n",
        "df = pd.DataFrame(employees)\n",
        "\n",
        "# Plot the distribution of the numerical values using a histogram\n",
        "fig, axs = plt.subplots(2,1, figsize=(10, 6))\n",
        "ax1, ax2 = axs\n",
        "\n",
        "ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\n",
        "ax1.set_xlabel('Year first hired')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Count the frequency of each category\n",
        "category_counts = df['department'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "category_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n",
        "\n",
        "# Add labels and title\n",
        "ax2.set_xlabel('Department')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n",
        "\n",
        "fig.suptitle(\"Distribution of values\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n",
        "```\n",
        "## Exploring the data {.smaller auto-animate=\"true\"}\n"
      ],
      "id": "8ea5aeab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import skrub\n",
        "from skrub.datasets import fetch_employee_salaries\n",
        "from pprint import pprint\n",
        "\n",
        "dataset = fetch_employee_salaries()\n",
        "employees, salaries = dataset.X, dataset.y\n",
        "\n",
        "df = pd.DataFrame(employees)\n",
        "\n",
        "# Plot the distribution of the numerical values using a histogram\n",
        "fig, axs = plt.subplots(2,1, figsize=(10, 6))\n",
        "ax1, ax2 = axs\n",
        "\n",
        "ax1.hist(df['year_first_hired'], bins=30, edgecolor='black', alpha=0.7)\n",
        "ax1.set_xlabel('Year first hired')\n",
        "ax1.set_ylabel('Frequency')\n",
        "ax1.grid(True, linestyle='--', alpha=0.5)\n",
        "\n",
        "# Count the frequency of each category\n",
        "category_counts = df['department'].value_counts()\n",
        "\n",
        "# Create a bar plot\n",
        "category_counts.plot(kind='bar', edgecolor='black', ax=ax2)\n",
        "\n",
        "# Add labels and title\n",
        "ax2.set_xlabel('Department')\n",
        "ax2.set_ylabel('Frequency')\n",
        "ax2.grid(True, linestyle='--', axis='y', alpha=0.5)  # Add grid lines for y-axis\n",
        "\n",
        "fig.suptitle(\"Distribution of values\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "id": "c120bd08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exploring the data with `skrub` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "from skrub import TableReport\n",
        "TableReport(employee_salaries)\n",
        "```\n",
        "[Preview](https://skrub-data.org/skrub-reports/examples/employee_salaries.html){preview-link=\"true\"}\n",
        "\n",
        "Main features:\n",
        "\n",
        "- Obtain high-level statistics about the data\n",
        "- Explore the distribution of values and find outliers\n",
        "- Discover highly correlated columns \n",
        "- Export and share the report as an `html` file\n",
        "\n",
        "\n",
        "::: {.fragment}\n",
        "<a href=\"https://skrub-data.org/skrub-reports/examples/\" target=\"_blank\">More examples</a>\n",
        "\n",
        ":::\n",
        "\n",
        "## Exploring the data with `skrub` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "Things to notice\n",
        "\n",
        "- Shape of the dataframe (rows, columns)\n",
        "- Dtype of the columns (string, numerical, datetime)\n",
        "    - Were dtypes converted properly? \n",
        "- Missing values\n",
        "    - Are there columns with many missing values? \n",
        "- Distribution of values\n",
        "    - Are there columns with high cardiinality? \n",
        "    - Are there outliers? \n",
        "    - Are there columns with imbalanced distributions? \n",
        "- Column associations\n",
        "    - Are there correlated columns? \n",
        "\n",
        "## Intermission: reading files, parsing dtypes\n",
        "- CSV vs Parquet+\n",
        "- Fun with separators\n",
        "- \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Data cleaning with pandas/polars: setup {.smaller auto-animate=\"true\"}\n",
        "\n",
        "::: {.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "388ae66a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "data = {\n",
        "    'Constant int': [1, 1, 1],  # Single unique value\n",
        "    'B': [2, 3, 2],  # Multiple unique values\n",
        "    'Constant str': ['x', 'x', 'x'],  # Single unique value\n",
        "    'D': [4, 5, 6],  # Multiple unique values\n",
        "    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n",
        "    'All empty': ['', '', ''],  # All empty strings\n",
        "    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n",
        "}\n",
        "\n",
        "df_pd = pd.DataFrame(data)\n",
        "display(df_pd)"
      ],
      "id": "f5fcf223",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "f2b8cf87"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import polars as pl\n",
        "import numpy as np\n",
        "data = {\n",
        "    'Constant int': [1, 1, 1],  # Single unique value\n",
        "    'B': [2, 3, 2],  # Multiple unique values\n",
        "    'Constant str': ['x', 'x', 'x'],  # Single unique value\n",
        "    'D': [4, 5, 6],  # Multiple unique values\n",
        "    'All nan': [np.nan, np.nan, np.nan],  # All missing values \n",
        "    'All empty': ['', '', ''],  # All empty strings\n",
        "    'Date': ['01/01/2023', '02/01/2023', '03/01/2023'],\n",
        "}\n",
        "\n",
        "df_pl = pl.DataFrame(data)\n",
        "display(df_pl)"
      ],
      "id": "e55ac677",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Nulls, datetimes, constant columns with pandas/polars {.smaller auto-animate=\"true\"}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "88479de0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# Parse the datetime strings with a specific format\n",
        "df_pd['Date'] = pd.to_datetime(df_pd['Date'], format='%d/%m/%Y')\n",
        "\n",
        "# Drop columns with only a single unique value\n",
        "df_pd_cleaned = df_pd.loc[:, df_pd.nunique(dropna=True) > 1]\n",
        "\n",
        "# Function to drop columns with only missing values or empty strings\n",
        "def drop_empty_columns(df):\n",
        "    # Drop columns with only missing values\n",
        "    df_cleaned = df.dropna(axis=1, how='all')\n",
        "    # Drop columns with only empty strings\n",
        "    empty_string_cols = df_cleaned.columns[df_cleaned.eq('').all()]\n",
        "    df_cleaned = df_cleaned.drop(columns=empty_string_cols)\n",
        "    return df_cleaned\n",
        "\n",
        "# Apply the function to the DataFrame\n",
        "df_pd_cleaned = drop_empty_columns(df_pd_cleaned)"
      ],
      "id": "796ed57e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "a93dbb08"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true \n",
        "# Parse the datetime strings with a specific format\n",
        "df_pl = df_pl.with_columns([\n",
        "    pl.col(\"Date\").str.strptime(pl.Date, \"%d/%m/%Y\", strict=False).alias(\"Date\")\n",
        "])\n",
        "\n",
        "# Drop columns with only a single unique value\n",
        "df_pl_cleaned = df_pl.select([\n",
        "    col for col in df_pl.columns if df_pl[col].n_unique() > 1\n",
        "])\n",
        "\n",
        "# Import selectors for dtype selection\n",
        "import polars.selectors as cs\n",
        "\n",
        "# Drop columns with only missing values or only empty strings\n",
        "def drop_empty_columns(df):\n",
        "    all_nan = df.select(\n",
        "        [\n",
        "            col for col in df.select(cs.numeric()).columns if \n",
        "            df [col].is_nan().all()\n",
        "        ]\n",
        "    ).columns\n",
        "    \n",
        "    all_empty = df.select(\n",
        "        [\n",
        "            col for col in df.select(cs.string()).columns if \n",
        "            (df[col].str.strip_chars().str.len_chars()==0).all()\n",
        "        ]\n",
        "    ).columns\n",
        "\n",
        "    to_drop = all_nan + all_empty\n",
        "\n",
        "    return df.drop(to_drop)\n",
        "\n",
        "df_pl_cleaned = drop_empty_columns(df_pl_cleaned)"
      ],
      "id": "98727b2b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Data cleaning with `skrub.Cleaner` {.smaller auto-animate=\"true\"}\n",
        "\n",
        ":::{.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "844618d7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pd)\n",
        "display(df_cleaned)"
      ],
      "id": "6b8f526c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "3d2cc1eb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import Cleaner\n",
        "cleaner = Cleaner(drop_if_constant=True, datetime_format='%d/%m/%Y')\n",
        "df_cleaned = cleaner.fit_transform(df_pl)\n",
        "display(df_cleaned)"
      ],
      "id": "b5b6fe37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Encoding datetime features with pandas/polars {.smaller}\n",
        ":::{.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "a7517541"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "import pandas as pd\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df_pd = pd.DataFrame(data)\n",
        "datetime_column = \"date\"\n",
        "df_pd[datetime_column] = pd.to_datetime(df_pd[datetime_column], errors='coerce')\n",
        "\n",
        "df_pd['year'] = df_pd[datetime_column].dt.year\n",
        "df_pd['month'] = df_pd[datetime_column].dt.month\n",
        "df_pd['day'] = df_pd[datetime_column].dt.day\n",
        "df_pd['hour'] = df_pd[datetime_column].dt.hour\n",
        "df_pd['minute'] = df_pd[datetime_column].dt.minute\n",
        "df_pd['second'] = df_pd[datetime_column].dt.second"
      ],
      "id": "82087283",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "267e8f1c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#|echo: true\n",
        "import polars as pl\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df_pl = pl.DataFrame(data)\n",
        "df_pl = df_pl.with_columns(date=pl.col(\"date\").str.to_datetime())\n",
        "\n",
        "df_pl = df_pl.with_columns(\n",
        "    year=pl.col(\"date\").dt.year(),\n",
        "    month=pl.col(\"date\").dt.month(),\n",
        "    day=pl.col(\"date\").dt.day(),\n",
        "    hour=pl.col(\"date\").dt.hour(),\n",
        "    minute=pl.col(\"date\").dt.minute(),\n",
        "    second=pl.col(\"date\").dt.second(),\n",
        ")"
      ],
      "id": "24ce3d5e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Adding periodic features with pandas/polars{.smaller}\n",
        ":::{.panel-tabset}\n",
        "\n",
        "### Pandas"
      ],
      "id": "95fd1294"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}"
      ],
      "id": "0e1e120d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df_pd['hour_sin'] = np.sin(2 * np.pi * df_pd['hour'] / 24)\n",
        "df_pd['hour_cos'] = np.cos(2 * np.pi * df_pd['hour'] / 24)\n",
        "\n",
        "df_pd['month_sin'] = np.sin(2 * np.pi * df_pd['month'] / 12)\n",
        "df_pd['month_cos'] = np.cos(2 * np.pi * df_pd['month'] / 12)"
      ],
      "id": "4003469d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "ced422cb"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df_pl = df_pl.with_columns(\n",
        "    hour_sin = np.sin(2 * np.pi * pl.col(\"hour\") / 24),\n",
        "    hour_cos = np.cos(2 * np.pi * pl.col(\"hour\") / 24),\n",
        "    \n",
        "    month_sin = np.sin(2 * np.pi * pl.col(\"month\") / 12),\n",
        "    month_cos = np.cos(2 * np.pi * pl.col(\"month\") / 12),\n",
        ")"
      ],
      "id": "5e91e46c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Encoding datetime features `skrub.DatetimeEncoder` {auto-animate=\"true\" visibility=\"uncounted\" .smaller}"
      ],
      "id": "574b4180"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "data = {\n",
        "    'date': ['2023-01-01 12:34:56', '2023-02-15 08:45:23', '2023-03-20 18:12:45'],\n",
        "    'value': [10, 20, 30]\n",
        "}\n",
        "df = pl.DataFrame(data)"
      ],
      "id": "d008a3ea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import DatetimeEncoder, ToDatetime\n",
        "\n",
        "X_date = ToDatetime().fit_transform(df[\"date\"])\n",
        "de = DatetimeEncoder(periodic_encoding=\"circular\")\n",
        "X_enc = de.fit_transform(X_date)\n",
        "print(X_enc)"
      ],
      "id": "0268c872",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## What periodic features look like\n",
        "![](images/periodic_features.png){fig-align=\"center\"}\n",
        "\n",
        "## Encoding numerical features with `skrub.SquashingScaler`"
      ],
      "id": "425f79f4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "np.random.seed(0)  # for reproducibility\n",
        "\n",
        "values = np.random.rand(100, 1)\n",
        "n_outliers = 15\n",
        "outlier_indices = np.random.choice(values.shape[0], size=n_outliers, replace=False)\n",
        "values[outlier_indices] = np.random.rand(n_outliers, 1) * 100 - 50\n",
        "\n",
        "x = np.arange(values.shape[0])\n",
        "fig, axs = plt.subplots(1, layout=\"constrained\", figsize=(6, 4))\n",
        "\n",
        "axs.plot(x,values)\n",
        "_ = axs.set(\n",
        "    title=\"Feature with outliers\",\n",
        "    ylabel=\"value\",\n",
        "    xlabel=\"Sample ID\"\n",
        "    )\n",
        "axs.axhspan(-2, 2, color=\"gray\", alpha=0.15)\n",
        "\n",
        "x_data, y_data = [30, 2]\n",
        "desc = \"Data is mostly\\nin [-2, 2]\"\n",
        "axs.annotate(\n",
        "    desc,\n",
        "    xy=(x_data, y_data),\n",
        "    xytext=(0.15, 0.8),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        "\n",
        ")\n",
        "\n",
        "x_outlier, y_outlier = np.argmax(values), np.max(values)\n",
        "desc = \"There are large\\noutliers throughout.\"\n",
        "_ = axs.annotate(\n",
        "    desc,\n",
        "    xy=(x_outlier, y_outlier),\n",
        "    xytext=(0.6, 0.85),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        "\n",
        ")"
      ],
      "id": "1fb38c16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding numerical features with `skrub.SquashingScaler`"
      ],
      "id": "3a49e5dc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-align: center\n",
        "from sklearn.preprocessing import QuantileTransformer, RobustScaler, StandardScaler\n",
        "from skrub import SquashingScaler\n",
        "\n",
        "squash_scaler = SquashingScaler()\n",
        "squash_scaled = squash_scaler.fit_transform(values)\n",
        "\n",
        "robust_scaler = RobustScaler()\n",
        "robust_scaled = robust_scaler.fit_transform(values)\n",
        "\n",
        "standard_scaler = StandardScaler()\n",
        "standard_scaled = standard_scaler.fit_transform(values)\n",
        "\n",
        "quantile_transformer = QuantileTransformer(n_quantiles=100)\n",
        "quantile_scaled = quantile_transformer.fit_transform(values)\n",
        "# %%\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x = np.arange(values.shape[0])\n",
        "fig, axs = plt.subplots(1, 2, layout=\"constrained\", figsize=(8, 5))\n",
        "\n",
        "ax = axs[0]\n",
        "ax.plot(x, sorted(values), label=\"Original Values\", linewidth=2.5)\n",
        "ax.plot(x, sorted(squash_scaled), label=\"SquashingScaler\")\n",
        "ax.plot(x, sorted(robust_scaled), label=\"RobustScaler\", linestyle=\"--\")\n",
        "ax.plot(x, sorted(standard_scaled), label=\"StandardScaler\")\n",
        "ax.plot(x, sorted(quantile_scaled), label=\"QuantileTransformer\")\n",
        "\n",
        "# Add a horizontal band in [-4, +4]\n",
        "ax.axhspan(-4, 4, color=\"gray\", alpha=0.15)\n",
        "ax.set(title=\"Original data\", xlim=[0, values.shape[0]], xlabel=\"Percentile\")\n",
        "ax.legend()\n",
        "\n",
        "ax = axs[1]\n",
        "ax.plot(x, sorted(values), label=\"Original Values\", linewidth=2.5)\n",
        "ax.plot(x, sorted(squash_scaled), label=\"SquashingScaler\")\n",
        "ax.plot(x, sorted(robust_scaled), label=\"RobustScaler\", linestyle=\"--\")\n",
        "ax.plot(x, sorted(standard_scaled), label=\"StandardScaler\")\n",
        "ax.plot(x, sorted(quantile_scaled), label=\"QuantileTransformer\")\n",
        "\n",
        "ax.set(ylim=[-4, 4])\n",
        "ax.set(title=\"In range [-4, 4]\", xlim=[0, values.shape[0]], xlabel=\"Percentile\")\n",
        "\n",
        "# Highlight the bounds of the SquashingScaler\n",
        "ax.axhline(y=3, alpha=0.2)\n",
        "ax.axhline(y=-3, alpha=0.2)\n",
        "\n",
        "fig.suptitle(\n",
        "    \"Comparison of different scalers on sorted data with outliers\", fontsize=20\n",
        ")\n",
        "fig.supylabel(\"Value\")\n",
        "\n",
        "desc = \"The RobustScaler is\\naffected by outliers\"\n",
        "axs[0].annotate(\n",
        "    desc,\n",
        "    xy=(0, -70),\n",
        "    xytext=(0.4, 0.2),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        ")\n",
        "\n",
        "desc = \"The SquashingScaler is\\nclipped to a finite value\"\n",
        "_ = axs[1].annotate(\n",
        "    desc,\n",
        "    xy=(0, -3),\n",
        "    xytext=(0.4, 0.2),\n",
        "    textcoords=\"axes fraction\",\n",
        "    arrowprops=dict(arrowstyle=\"->\", color=\"red\"),\n",
        ")"
      ],
      "id": "460e6591",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Encoding categorical (string/text) features {.smaller}\n",
        "Categorical features have a \"cardinality\": the number of unique values\n",
        "\n",
        "- Low cardinality features: `OneHotEncoder`\n",
        "    - The `OneHotEncoder` produces **sparse** matrices. Dataframes are **dense**.\n",
        "- High cardinality features (>40 unique values): \n",
        "    - Using `OneHotEncoder` generates too many (dense) features.\n",
        "    - Use encoders with a fixed number of output features.\n",
        "- [Latent Semantic Analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) with: `skrub.StringEncoder`\n",
        "    - Apply tf-idf to the ngrams in the column, followed by SVD. \n",
        "    - Robust, quick, fixed number of output features regardless of # of unique values. \n",
        "\n",
        "## Encoding categorical (string/text) features {.smaller}\n",
        "\n",
        "- Textual features: `skrub.TextEncoder` and pretrained models from [HuggingFace Hub](https://huggingface.co/models).\n",
        "    - The `TextEncoder` needs Torch: _very heavy dependency_. \n",
        "    - Models are large and take time to download. \n",
        "    - Encoding is much slower.\n",
        "    - However, performance can be much better depending on the dataset. \n",
        "\n",
        "Deeper dive in this [post](https://skrub-data.org/skrub-materials/pages/notebooks/categorical-encoders/categorical-encoders.html)\n",
        "\n",
        "## Encoding _all the features_: `TableVectorizer` { auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "from skrub import TableVectorizer, TextEncoder\n",
        "\n",
        "text = TextEncoder()\n",
        "table_vec = TableVectorizer(high_cardinality=text)\n",
        "df_encoded = table_vec.fit_transform(df)\n",
        "\n",
        "```\n",
        "::: {.fragment}\n",
        "- Apply the `Cleaner` to all columns\n",
        "- Split columns by dtype and # of unique values\n",
        "- Encode each column separately\n",
        ":::\n",
        "\n",
        "\n",
        "## Encoding _all the features_: `TableVectorizer` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "![](images/skrub-table-vectorizer.png)\n",
        "\n",
        "## Column transformations with `ApplyToCols` and `ApplyToFrame` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "![](images/ApplyToCols.png)\n",
        "\n",
        "## Column transformations with `ApplyToCols` and `ApplyToFrame` {.smaller auto-animate=\"true\"}\n",
        "\n",
        "![](images/ApplyToFrame.png)\n",
        "\n",
        "## Replacing `ColumnTransformer` with `ApplyToCols` {.smaller auto-animate=\"true\"}\n"
      ],
      "id": "ea3809fc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import pandas as pd\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.compose import make_column_transformer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "\n",
        "df = pd.DataFrame({\"text\": [\"foo\", \"bar\", \"baz\"], \"number\": [1, 2, 3]})\n",
        "\n",
        "categorical_columns = selector(dtype_include=object)(df)\n",
        "numerical_columns = selector(dtype_exclude=object)(df)\n",
        "\n",
        "ct = make_column_transformer(\n",
        "      (StandardScaler(),\n",
        "       numerical_columns),\n",
        "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
        "       categorical_columns))\n",
        "transformed = ct.fit_transform(df)\n",
        "transformed"
      ],
      "id": "d68d1a0c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Replacing `ColumnTransformer` with `ApplyToCols` {.smaller auto-animate=\"true\"}"
      ],
      "id": "2729dfb0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub.selectors as s\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from skrub import ApplyToCols\n",
        "\n",
        "numeric = ApplyToCols(StandardScaler(), cols=s.numeric())\n",
        "string = ApplyToCols(OneHotEncoder(sparse_output=False), cols=s.string())\n",
        "\n",
        "transformed = make_pipeline(numeric, string).fit_transform(df)\n",
        "transformed"
      ],
      "id": "30fde748",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting columns with the `skrub.selectors` {.smaller}\n",
        "\n",
        "- \n",
        "\n",
        "\n",
        "\n",
        "## Let's build a predictive pipeline {auto-animate=\"true\"}\n",
        "```{.python}\n",
        "from sklearn.linear_model import Ridge\n",
        "model = Ridge()\n",
        "```\n",
        "\n",
        "## Let's build a predictive pipeline {auto-animate=\"true\" visibility=\"uncounted\"}\n",
        "```{.python code-line-numbers=\"3-6\"}\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "model = make_pipeline(StandardScaler(), SimpleImputer(), Ridge())\n",
        "```\n",
        "\n",
        "\n",
        "## Let's build a predictive pipeline {auto-animate=\"true\" visibility=\"uncounted\"}\n",
        "```{.python code-line-numbers=\"3,5,6-17|\"}\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import make_column_selector as selector\n",
        "from sklearn.compose import make_column_transformer\n",
        "\n",
        "categorical_columns = selector(dtype_include=object)(employees)\n",
        "numerical_columns = selector(dtype_exclude=object)(employees)\n",
        "\n",
        "ct = make_column_transformer(\n",
        "      (StandardScaler(),\n",
        "       numerical_columns),\n",
        "      (OneHotEncoder(handle_unknown=\"ignore\"),\n",
        "       categorical_columns))\n",
        "\n",
        "model = make_pipeline(ct, SimpleImputer(), Ridge())\n",
        "```\n",
        "## Now with `tabular_pipeline` {auto-animate=\"true\" .smaller}"
      ],
      "id": "5c460375"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub\n",
        "from sklearn.linear_model import Ridge\n",
        "model = skrub.tabular_pipeline(Ridge())"
      ],
      "id": "618aa07b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![](images/skrub-tabular-pipeline-linear-model.png){fig-align=\"center\"}\n",
        "\n",
        "## \n",
        "![](images/drakeno.png){fig-align=\"center\"}\n",
        "\n",
        "## We now have a pipeline! {.smaller}\n",
        "\n",
        "1. Gather some data\n",
        "2. Explore the data\n",
        "    - `skrub.TableReport`\n",
        "3. Pre-process the data \n",
        "    - `Cleaner`, `ToDatetime` ... \n",
        "4. Perform feature engineering\n",
        "    - `skrub.TableVectorizer`,`SquashingScaler`, `TextEncoder`, `StringEncoder `...\n",
        "5. Build a scikit-learn pipeline\n",
        "    - `tabular_pipeline`, `sklearn.pipeline.make_pipeline` ... \n",
        "6. ???\n",
        "7. Profit ðŸ“ˆ \n",
        "\n",
        "\n",
        "# What if this is not enough?? \n",
        "\n",
        "## What if...\n",
        "\n",
        "- Your data is spread over multiple tables? \n",
        "- You want to avoid data leakage? \n",
        "- You want to tune more than just the hyperparameters of your model? \n",
        "- You want to guarantee that your pipeline is replayed exactly on new data? \n",
        "\n",
        "## \n",
        "When a normal pipe is not enough...\n",
        "\n",
        "::: {.fragment style=\"font-size:2em;\"}\n",
        "... the `skrub` DataOps come to the rescue ðŸš’\n",
        ":::\n",
        "\n",
        "\n",
        "## DataOps...\n",
        "- Extend the `scikit-learn` machinery to complex multi-table operations, and take care of data leakage\n",
        "- Track all operations with a computational graph (a *Data Ops plan*)\n",
        "- Allow tuning any operation in the data plan\n",
        "- Can be persisted and shared easily \n",
        "\n",
        "## How do DataOps work, though?  {.smaller}\n",
        "DataOps **wrap** around *user operations*, where user operations are:\n",
        "\n",
        "- any dataframe operation (e.g., merge, group by, aggregate etc.)\n",
        "- scikit-learn estimators (a Random Forest, RidgeCV etc.)\n",
        "- custom user code (load data from a path, fetch from an URL etc.)\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "::: {.callout-important}\n",
        "DataOps _record_ user operations, so that they can later be _replayed_ in the same\n",
        "order and with the same arguments on unseen data. \n",
        ":::\n",
        "::: \n",
        "\n",
        "## DataOps, Plans, `learner`s: oh my!  \n",
        "- A `DataOp` (singular) wraps a single operation, and can be combined and concatenated with other `DataOps`. \n",
        "\n",
        "- The **Data Ops** Plan is a collective name for the directed computational graph\n",
        "that tracks a sequence and combination of `DataOps`. \n",
        "\n",
        "- The plan can be exported as a standalone object called `learner`. The `learner` \n",
        "works like a scikit-learn estimator that takes a dictionary of values rather \n",
        "than just `X` and `y`. \n",
        "\n",
        "## Starting with the `DataOps`\n"
      ],
      "id": "b0913713"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "import skrub\n",
        "data = skrub.datasets.fetch_credit_fraud()\n",
        "\n",
        "baskets = skrub.var(\"baskets\", data.baskets)\n",
        "products = skrub.var(\"products\", data.products) # add a new variable\n",
        "\n",
        "X = baskets[[\"ID\"]].skb.mark_as_X()\n",
        "y = baskets[\"fraud_flag\"].skb.mark_as_y()"
      ],
      "id": "898559e4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::{.incremental}\n",
        "- `X`, `y`, `products` represent inputs to the pipeline.\n",
        "- `skrub` splits `X` and `y` when training. \n",
        ":::\n",
        "\n",
        "##  Building a full data plan"
      ],
      "id": "6bffa453"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from skrub import selectors as s\n",
        "\n",
        "vectorizer = skrub.TableVectorizer(\n",
        "    high_cardinality=skrub.StringEncoder()\n",
        ")\n",
        "vectorized_products = products.skb.apply(vectorizer, cols=s.all() - \"basket_ID\")"
      ],
      "id": "14f3451a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Building a full data plan {auto-animate=\"true\"}"
      ],
      "id": "d773e8b4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "aggregated_products = vectorized_products.groupby(\n",
        "    \"basket_ID\"\n",
        ").agg(\"mean\").reset_index()\n",
        "\n",
        "features = X.merge(aggregated_products, left_on=\"ID\", right_on=\"basket_ID\")\n",
        "features = features.drop(columns=[\"ID\", \"basket_ID\"])"
      ],
      "id": "06e81266",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##  Building a full data plan {auto-animate=\"true\"}"
      ],
      "id": "065ca962"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "from sklearn.ensemble import ExtraTreesClassifier  \n",
        "predictions = features.skb.apply(\n",
        "    ExtraTreesClassifier(n_jobs=-1), y=y\n",
        ")"
      ],
      "id": "b3b4e13d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the data plan\n",
        "```{.python}\n",
        "predictions.skb.full_report()\n",
        "```\n",
        "<br/>\n",
        "\n",
        "<a href=\"dataop_report/index.html\" target=\"_blank\">Execution report</a>\n",
        "\n",
        "Each node:\n",
        "\n",
        "- Shows a preview of the data resulting from the operation\n",
        "- Reports the location in the code where the code is defined\n",
        "- Shows the run time of the node (in the next release)\n",
        "\n",
        "## Exporting the plan in a `learner` {.smaller}\n",
        "The data plan can be exported as a `learner`:"
      ],
      "id": "ba3fc343"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "# anywhere\n",
        "learner = predictions.skb.make_learner(fitted=True)"
      ],
      "id": "32a71a16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "::: {.fragment}\n",
        "Then, the `learner` can be pickled ...\n"
      ],
      "id": "517e7e9e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pickle \n",
        "\n",
        "learner_bytes = pickle.dumps(learner)"
      ],
      "id": "26cc229f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{.python}\n",
        "import pickle\n",
        "\n",
        "with open(\"learner.bin\", \"wb\") as fp:\n",
        "    pickle.dump(learner, fp)\n",
        "```\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "\n",
        "... loaded ...\n"
      ],
      "id": "e4b54e52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "loaded_learner = pickle.loads(learner_bytes)"
      ],
      "id": "596a5714",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```{.python}\n",
        "with open(\"learner.bin\", \"rb\") as fp:\n",
        "    loaded_learner = pickle.load(fp)\n",
        "```\n",
        ":::\n",
        "\n",
        "::: {.fragment}\n",
        "... and applied to new data:"
      ],
      "id": "6eac2d8a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "data = skrub.datasets.fetch_credit_fraud(split=\"test\")\n",
        "new_baskets = data.baskets\n",
        "new_products = data.products\n",
        "loaded_learner.predict({\"baskets\": new_baskets, \"products\": new_products})"
      ],
      "id": "21fffc4f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "\n",
        "## Hyperparameter tuning in a Data Plan \n",
        "`skrub` implements four `choose_*` functions:\n",
        "\n",
        "- `choose_from`: select from the given list of options\n",
        "- `choose_int`: select an integer within a range\n",
        "- `choose_float`: select a float within a range\n",
        "- `choose_bool`: select a bool \n",
        "- `optional`: chooses whether to execute the given operation\n",
        "\n",
        "\n",
        "## Tuning in `scikit-learn` can be complex {.smaller auto-animate=\"true\"}\n",
        "\n",
        "```{.python}\n",
        "pipe = Pipeline([(\"dim_reduction\", PCA()), (\"regressor\", Ridge())])\n",
        "grid = [\n",
        "    {\n",
        "        \"dim_reduction\": [PCA()],\n",
        "        \"dim_reduction__n_components\": [10, 20, 30],\n",
        "        \"regressor\": [Ridge()],\n",
        "        \"regressor__alpha\": loguniform(0.1, 10.0),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [SelectKBest()],\n",
        "        \"dim_reduction__k\": [10, 20, 30],\n",
        "        \"regressor\": [Ridge()],\n",
        "        \"regressor__alpha\": loguniform(0.1, 10.0),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [PCA()],\n",
        "        \"dim_reduction__n_components\": [10, 20, 30],\n",
        "        \"regressor\": [RandomForestClassifier()],\n",
        "        \"regressor__n_estimators\": loguniform(20, 200),\n",
        "    },\n",
        "    {\n",
        "        \"dim_reduction\": [SelectKBest()],\n",
        "        \"dim_reduction__k\": [10, 20, 30],\n",
        "        \"regressor\": [RandomForestClassifier()],\n",
        "        \"regressor__n_estimators\": loguniform(20, 200),\n",
        "    },\n",
        "]\n",
        "model = RandomizedSearchCV(pipe, grid)\n",
        "```\n",
        "## Tuning with `DataOps` is simple! {.smaller} \n",
        "\n",
        "```python\n",
        "dim_reduction = X.skb.apply(\n",
        "    skrub.choose_from(\n",
        "        {\n",
        "            \"PCA\": PCA(n_components=skrub.choose_int(10, 30)),\n",
        "            \"SelectKBest\": SelectKBest(k=skrub.choose_int(10, 30))\n",
        "        }, name=\"dim_reduction\"\n",
        "    )\n",
        ")\n",
        "regressor = dim_reduction.skb.apply(\n",
        "    skrub.choose_from(\n",
        "        {\n",
        "            \"Ridge\": Ridge(alpha=skrub.choose_float(0.1, 10.0, log=True)),\n",
        "            \"RandomForest\": RandomForestClassifier(\n",
        "                n_estimators=skrub.choose_int(20, 200, log=True)\n",
        "            )\n",
        "        }, name=\"regressor\"\n",
        "    )\n",
        ")\n",
        "search = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True)\n",
        "```\n",
        "\n",
        "## Tuning with `DataOps` is not limited to estimators\n",
        "::: {.panel-tabset}\n",
        "### Pandas"
      ],
      "id": "337bdef1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import skrub"
      ],
      "id": "5a8e5c78",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df = pd.DataFrame(\n",
        "    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n",
        ")\n",
        "\n",
        "df_do = skrub.var(\"grades\", df)\n",
        "\n",
        "agg_grades = df_do.groupby(\"subject\").agg(skrub.choose_from([\"count\", \"mean\"]))\n",
        "agg_grades.skb.describe_param_grid()"
      ],
      "id": "3dc0a117",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Polars"
      ],
      "id": "1b26f89b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import polars as pl\n",
        "import skrub"
      ],
      "id": "0d9c725d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "df = pl.DataFrame(\n",
        "    {\"subject\": [\"math\", \"math\", \"art\", \"history\"], \"grade\": [10, 8, 4, 6]}\n",
        ")\n",
        "\n",
        "df_do = skrub.var(\"grades\", df)\n",
        "\n",
        "agg_grades = df_do.group_by(\"subject\").agg(\n",
        "    skrub.choose_from([pl.mean(\"grade\"), pl.count(\"grade\")])\n",
        ")\n",
        "agg_grades.skb.describe_param_grid()"
      ],
      "id": "8ce7d456",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        ":::\n",
        "\n",
        "## Run hyperparameter search\n",
        "```{.python}\n",
        "# fit the search \n",
        "search = regressor.skb.make_randomized_search(scoring=\"roc_auc\", fitted=True, cv=5)\n",
        "\n",
        "# save the best learner\n",
        "best_learner = search.best_learner_\n",
        "```\n",
        "\n",
        "## Observe the impact of the hyperparameters {auto-animate=\"true\" .smaller} \n",
        "Data Ops provide a built-in parallel coordinate plot. \n",
        "\n",
        "```{.python}\n",
        "search = pred.skb.get_randomized_search(fitted=True)\n",
        "search.plot_parallel_coord()\n",
        "```"
      ],
      "id": "c08ba907"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from plotly.io import read_json\n",
        "\n",
        "fig = read_json(\"parallel_coordinates_hgbr.json\")\n",
        "fig.update_layout(margin=dict(l=200))"
      ],
      "id": "5495b971",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[source](https://skrub-data.org/EuroSciPy2025/content/notebooks/single_horizon_prediction.html)\n",
        "\n",
        "## More information about the Data Ops \n",
        "- Skrub [example gallery](https://skrub-data.org/stable/auto_examples/data_ops/index.html)\n",
        "- [Tutorial](https://github.com/skrub-data/EuroSciPy2025) on timeseries \n",
        "forecasting at Euroscipy 2025\n",
        "- Skrub [User guide](https://skrub-data.org/stable/documentation.html)\n",
        "- A [Kaggle notebook](https://www.kaggle.com/code/ryye107/titanic-challenge-with-the-skrub-data-ops) \n",
        "on addressing the Titanic survival challenge with Data Ops\n",
        "\n",
        "# Wrapping up\n",
        "\n",
        "## {auto-animate=\"true\" } \n",
        "![](images/powerpuff_girls_1.png)\n",
        "\n",
        "## {auto-animate=\"true\" } \n",
        "![](images/powerpuff_girls_2.png)\n",
        "\n",
        "##  Getting involved {.smaller}\n",
        "::: {.nonincremental}\n",
        "Do you want to learn more? \n",
        "\n",
        "- [Skrub website](https://skrub-data.org/stable/) \n",
        "- [Skrub materials website](https://skrub-data.org/skrub-materials/index.html)\n",
        "- [Discord server](https://discord.gg/ABaPnm7fDC)\n",
        "\n",
        "Follow skrub on:\n",
        "\n",
        "- [Bluesky](https://bsky.app/profile/skrub-data.bsky.social)\n",
        "- [LinkedIn](https://www.linkedin.com/company/skrub-data/)\n",
        "\n",
        "Star skrub on GitHub, or contribute directly: \n",
        "\n",
        "- [Git repository](https://github.com/skrub-data/skrub/)\n",
        ":::\n",
        "\n",
        "## tl;dw\n",
        "`skrub` provides\n",
        "\n",
        "::: {.nonincremental}\n",
        "- interactive data exploration\n",
        "- automated pre-processing of pandas and polars dataframes\n",
        "- powerful feature engineering\n",
        "- DataOps, plans, hyperparameter tuning, (almost) no leakage \n",
        ":::"
      ],
      "id": "7650625a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/rcap/Library/Python/3.9/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}